{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import zoom, rotate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/raghuram/ARPL/MR-Image-Reconstruction-Using-Deep-Learning/Task02_Heart\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(path):\n",
    "    \"\"\"\n",
    "    Load a .nii.gz file using nibabel and return:\n",
    "      - data (as a np.ndarray)\n",
    "      - affine (4x4 transform matrix)\n",
    "      - header (NIfTI header)\n",
    "    \"\"\"\n",
    "    nifti = nib.load(path)\n",
    "    data = nifti.get_fdata(dtype=np.float32)  # or np.float64 if you prefer\n",
    "    affine = nifti.affine\n",
    "    header = nifti.header\n",
    "    return data, affine, header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacing_from_affine(affine):\n",
    "    \"\"\"\n",
    "    Given a 4x4 affine matrix from nibabel, extract voxel spacing\n",
    "    along each dimension: (z, y, x).\n",
    "    \"\"\"\n",
    "    # Typically, the diagonal or row vectors of the affine (in absolute value)\n",
    "    # correspond to voxel size. A simple approach:\n",
    "    sx = np.sqrt(affine[0, 0]**2 + affine[0, 1]**2 + affine[0, 2]**2)\n",
    "    sy = np.sqrt(affine[1, 0]**2 + affine[1, 1]**2 + affine[1, 2]**2)\n",
    "    sz = np.sqrt(affine[2, 0]**2 + affine[2, 1]**2 + affine[2, 2]**2)\n",
    "    return np.array([sz, sy, sx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 4\n",
    "\n",
    "def get_spacing_from_affine(affine):\n",
    "    \"\"\"\n",
    "    Given a 4x4 affine matrix from nibabel, extract the voxel spacing\n",
    "    along each dimension (z, y, x).\n",
    "    \"\"\"\n",
    "    sx = np.sqrt(affine[0, 0]**2 + affine[0, 1]**2 + affine[0, 2]**2)\n",
    "    sy = np.sqrt(affine[1, 0]**2 + affine[1, 1]**2 + affine[1, 2]**2)\n",
    "    sz = np.sqrt(affine[2, 0]**2 + affine[2, 1]**2 + affine[2, 2]**2)\n",
    "    return np.array([sz, sy, sx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 5\n",
    "\n",
    "def resample_volume(volume, current_spacing, new_spacing, is_label=False, order=1):\n",
    "    \"\"\"\n",
    "    Resample a 3D volume (or label) to new_spacing using scipy.ndimage.zoom.\n",
    "      - volume: np.ndarray of shape (z, y, x).\n",
    "      - current_spacing: array-like, e.g. [sz, sy, sx].\n",
    "      - new_spacing: array-like, e.g. [nz, ny, nx].\n",
    "      - is_label: If True, use nearest neighbor for labels (order=0).\n",
    "      - order: interpolation order for images (1=linear by default).\n",
    "\n",
    "    Returns the resampled volume as np.ndarray.\n",
    "    \"\"\"\n",
    "    if is_label:\n",
    "        order = 0  # nearest-neighbor for segmentations\n",
    "\n",
    "    zoom_factors = current_spacing / new_spacing\n",
    "    resampled = zoom(volume, zoom=zoom_factors, order=order)\n",
    "    return resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 6\n",
    "\n",
    "def min_max_scale_intensity(img, min_val=-57, max_val=164, clip=True):\n",
    "    \"\"\"\n",
    "    Scale intensity to [0, 1] range, assuming raw intensities\n",
    "    lie roughly in [min_val, max_val]. Optionally clip to [0,1].\n",
    "    \"\"\"\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - min_val) / (max_val - min_val)\n",
    "    if clip:\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 8\n",
    "\n",
    "def random_rotate90_3d(img, prob=0.5, max_k=3):\n",
    "    \"\"\"\n",
    "    Randomly rotate the 3D image by 90, 180, or 270 degrees (k=1..max_k)\n",
    "    around a random plane of axes, with probability prob.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < prob:\n",
    "        k = np.random.randint(1, max_k + 1)  # 1, 2, or 3\n",
    "        axis_pairs = [(0,1), (1,2), (0,2)]    # e.g., rotate in XY, YZ, or XZ plane\n",
    "        axes = axis_pairs[np.random.randint(len(axis_pairs))]\n",
    "        img = np.rot90(img, k=k, axes=axes)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 9\n",
    "\n",
    "def random_zoom_3d(img, prob=0.2, min_zoom=0.9, max_zoom=1.1, order=1, is_label=False):\n",
    "    \"\"\"\n",
    "    Randomly zoom a 3D volume in/out with a factor in [min_zoom, max_zoom],\n",
    "    with probability prob. If is_label=True, uses nearest neighbor.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < prob:\n",
    "        zf = np.random.uniform(min_zoom, max_zoom, size=3)\n",
    "        if is_label:\n",
    "            order = 0  # nearest for labels\n",
    "        img = zoom(img, zoom=zf, order=order)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 20/20 [00:23<00:00,  1.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>shape_img</th>\n",
       "      <th>shape_lbl</th>\n",
       "      <th>img_min</th>\n",
       "      <th>img_max</th>\n",
       "      <th>lbl_min</th>\n",
       "      <th>lbl_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 326, 129)</td>\n",
       "      <td>(351, 326, 129)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(110, 320, 351)</td>\n",
       "      <td>(110, 320, 351)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(130, 320, 351)</td>\n",
       "      <td>(130, 320, 351)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 100)</td>\n",
       "      <td>(351, 320, 100)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 120, 320)</td>\n",
       "      <td>(351, 120, 320)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 120, 320)</td>\n",
       "      <td>(351, 120, 320)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 90)</td>\n",
       "      <td>(351, 320, 90)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(383, 334, 131)</td>\n",
       "      <td>(383, 334, 131)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         image_path  \\\n",
       "0      0  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "1      1  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "2      2  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "3      3  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "4      4  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "5      5  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "6      6  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "7      7  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "8      8  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "9      9  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "\n",
       "                                          label_path        shape_img  \\\n",
       "0  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 326, 129)   \n",
       "1  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (110, 320, 351)   \n",
       "2  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 120)   \n",
       "3  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (130, 320, 351)   \n",
       "4  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 100)   \n",
       "5  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 120, 320)   \n",
       "6  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 120, 320)   \n",
       "7  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 120)   \n",
       "8  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   (351, 320, 90)   \n",
       "9  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (383, 334, 131)   \n",
       "\n",
       "         shape_lbl   img_min  img_max  lbl_min  lbl_max  \n",
       "0  (351, 326, 129)  0.257919      1.0      0.0      1.0  \n",
       "1  (110, 320, 351)  0.257919      1.0      0.0      1.0  \n",
       "2  (351, 320, 120)  0.257919      1.0      0.0      1.0  \n",
       "3  (130, 320, 351)  0.257919      1.0      0.0      1.0  \n",
       "4  (351, 320, 100)  0.257919      1.0      0.0      1.0  \n",
       "5  (351, 120, 320)  0.257919      1.0      0.0      1.0  \n",
       "6  (351, 120, 320)  0.257919      1.0      0.0      1.0  \n",
       "7  (351, 320, 120)  0.257919      1.0      0.0      1.0  \n",
       "8   (351, 320, 90)  0.257919      1.0      0.0      1.0  \n",
       "9  (383, 334, 131)  0.257919      1.0      0.0      1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jupyter Cell 10\n",
    "\n",
    "# 1) Gather file paths\n",
    "images = sorted(glob(os.path.join(ROOT_DIR, \"imagesTr\", \"*.nii.gz\")))\n",
    "labels = sorted(glob(os.path.join(ROOT_DIR, \"labelsTr\", \"*.nii.gz\")))\n",
    "\n",
    "# 2) Set a desired target spacing (z, y, x)\n",
    "target_spacing = np.array([1.25, 1.25, 1.25])\n",
    "\n",
    "# 3) Prepare a list for stats\n",
    "stats_list = []\n",
    "\n",
    "# 4) Loop over all images/labels\n",
    "for idx, (img_path, lbl_path) in enumerate(tqdm(zip(images, labels), \n",
    "                                                total=len(images),\n",
    "                                                desc=\"Processing\")):\n",
    "    # --- Load\n",
    "    img, aff_img, _ = load_nifti(img_path)\n",
    "    lbl, aff_lbl, _ = load_nifti(lbl_path)\n",
    "    \n",
    "    # --- Original spacings\n",
    "    orig_spacing_img = get_spacing_from_affine(aff_img)\n",
    "    orig_spacing_lbl = get_spacing_from_affine(aff_lbl)  # often the same as img\n",
    "    \n",
    "    # --- Resample\n",
    "    img_rs = resample_volume(img, orig_spacing_img, target_spacing, is_label=False, order=1)\n",
    "    lbl_rs = resample_volume(lbl, orig_spacing_lbl, target_spacing, is_label=True)\n",
    "    \n",
    "    # --- Intensity normalization (image only)\n",
    "    img_scaled = min_max_scale_intensity(img_rs, min_val=-57, max_val=164, clip=True)\n",
    "    \n",
    "    # --- Data augmentation\n",
    "    # Must apply the same random transforms to both image & label\n",
    "\n",
    "    # Random flip\n",
    "    if np.random.rand() < 0.5:\n",
    "        flip_axis = np.random.choice([0,1,2])\n",
    "        img_scaled = np.flip(img_scaled, axis=flip_axis)\n",
    "        lbl_rs = np.flip(lbl_rs, axis=flip_axis)\n",
    "\n",
    "    # Random rotate 90\n",
    "    if np.random.rand() < 0.5:\n",
    "        k = np.random.randint(1, 4)  # 1, 2, or 3\n",
    "        axis_pairs = [(0,1), (1,2), (0,2)]\n",
    "        ax = axis_pairs[np.random.randint(len(axis_pairs))]\n",
    "        img_scaled = np.rot90(img_scaled, k=k, axes=ax)\n",
    "        lbl_rs = np.rot90(lbl_rs, k=k, axes=ax)\n",
    "\n",
    "    # Random zoom\n",
    "    if np.random.rand() < 0.2:\n",
    "        zf = np.random.uniform(0.9, 1.1, size=3)\n",
    "        img_scaled = zoom(img_scaled, zoom=zf, order=1)\n",
    "        lbl_rs = zoom(lbl_rs, zoom=zf, order=0)\n",
    "\n",
    "    # --- Collect stats\n",
    "    img_min, img_max = float(img_scaled.min()), float(img_scaled.max())\n",
    "    lbl_min, lbl_max = float(lbl_rs.min()), float(lbl_rs.max())\n",
    "\n",
    "    stats_list.append({\n",
    "        \"index\": idx,\n",
    "        \"image_path\": img_path,\n",
    "        \"label_path\": lbl_path,\n",
    "        \"shape_img\": img_scaled.shape,\n",
    "        \"shape_lbl\": lbl_rs.shape,\n",
    "        \"img_min\": img_min,\n",
    "        \"img_max\": img_max,\n",
    "        \"lbl_min\": lbl_min,\n",
    "        \"lbl_max\": lbl_max\n",
    "    })\n",
    "\n",
    "# 5) Convert stats to a DataFrame\n",
    "df_stats = pd.DataFrame(stats_list)\n",
    "df_stats.to_csv(\"heart_dataset_stats.csv\", index=False)\n",
    "df_stats.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, target_spacing=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths  (list): list of paths to the NIfTI images.\n",
    "            label_paths  (list): list of paths to the corresponding NIfTI labels.\n",
    "            target_spacing (np.array): optional, if you plan to resample images.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.target_spacing = target_spacing  # not used here, but can be used for resampling\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        lbl_path = self.label_paths[idx]\n",
    "\n",
    "        # Load NIfTI images (shape: [Z, Y, X] typically)\n",
    "        img_nifti = nib.load(img_path)\n",
    "        lbl_nifti = nib.load(lbl_path)\n",
    "\n",
    "        img = img_nifti.get_fdata(dtype=np.float32)\n",
    "        lbl = lbl_nifti.get_fdata(dtype=np.float32)\n",
    "\n",
    "        # If you have specific transforms (normalization, cropping, etc.), apply them here\n",
    "        # e.g. min-max normalization, etc.\n",
    "\n",
    "        # Expand dims to have shape (1, Z, Y, X) for PyTorch\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        lbl = np.expand_dims(lbl, axis=0)\n",
    "\n",
    "        return torch.from_numpy(img), torch.from_numpy(lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = HeartDataset(\n",
    "    image_paths=images,\n",
    "    label_paths=labels,\n",
    "    target_spacing=np.array([1.25, 1.25, 1.25]),)\n",
    "\n",
    "# Create DataLoader\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    A helper module that performs a 3D convolution -> ReLU -> 3D convolution -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Downscaling (via MaxPool3D) followed by a DoubleConv3D.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling then a DoubleConv3D. We can do either:\n",
    "      - Transposed Conv if bilinear=False\n",
    "      - nn.Upsample (trilinear) if bilinear=True\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "        # If using bilinear upsampling, we keep the number of parameters lower:\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            # Deconvolution / Transposed Conv\n",
    "            self.up = nn.ConvTranspose3d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        # A DoubleConv3D after concatenation\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is the \"decoder\" feature map, x2 is the \"skip connection\" from encoder\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # We may need to pad x1 to match x2's size\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2])\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3D UNet with 4 down-sampling / up-sampling levels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=32, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv3D(in_channels, base_filters)\n",
    "        self.down1 = Down3D(base_filters, base_filters * 2)\n",
    "        self.down2 = Down3D(base_filters * 2, base_filters * 4)\n",
    "        self.down3 = Down3D(base_filters * 4, base_filters * 8)\n",
    "        # If bilinear upsampling, we can reduce the number of filters by a factor of 2 in the bottleneck\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down3D(base_filters * 8, base_filters * 16 // factor)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = Up3D(base_filters * 16, base_filters * 8 // factor, bilinear)\n",
    "        self.up2 = Up3D(base_filters * 8, base_filters * 4 // factor, bilinear)\n",
    "        self.up3 = Up3D(base_filters * 4, base_filters * 2 // factor, bilinear)\n",
    "        self.up4 = Up3D(base_filters * 2, base_filters, bilinear)\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        self.outc = nn.Conv3d(base_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder path\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop_3d(img, lbl, crop_size=(64, 128, 128)):\n",
    "    \"\"\"\n",
    "    Randomly crop a 3D patch of size crop_size from img and lbl.\n",
    "    Assume img, lbl shape: (Z, Y, X).\n",
    "    \"\"\"\n",
    "    z, y, x = img.shape\n",
    "    cz, cy, cx = crop_size\n",
    "    \n",
    "    # pick random start\n",
    "    z0 = np.random.randint(0, z - cz) if z > cz else 0\n",
    "    y0 = np.random.randint(0, y - cy) if y > cy else 0\n",
    "    x0 = np.random.randint(0, x - cx) if x > cx else 0\n",
    "    \n",
    "    img_patch = img[z0:z0+cz, y0:y0+cy, x0:x0+cx]\n",
    "    lbl_patch = lbl[z0:z0+cz, y0:y0+cy, x0:x0+cx]\n",
    "    return img_patch, lbl_patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13684/3913200588.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # handles dynamic loss scaling for FP16\n",
      "/tmp/ipykernel_13684/3913200588.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.75 GiB of which 2.41 GiB is free. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 133.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m finished. Average loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 40\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Use autocast for forward + loss computation\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: (B, 2, Z, Y, X)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Backprop with scaled gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 114\u001b[0m, in \u001b[0;36mUNet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x, x3)\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x, x2)\n\u001b[0;32m--> 114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutc(x)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 71\u001b[0m, in \u001b[0;36mUp3D.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     66\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x1, [diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffX \u001b[38;5;241m-\u001b[39m diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     67\u001b[0m                 diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffY \u001b[38;5;241m-\u001b[39m diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     68\u001b[0m                 diffZ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffZ \u001b[38;5;241m-\u001b[39m diffZ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Concatenate along channel dimension\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.93 GiB. GPU 0 has a total capacity of 7.75 GiB of which 2.41 GiB is free. Including non-PyTorch memory, this process has 5.23 GiB memory in use. Of the allocated memory 4.96 GiB is allocated by PyTorch, and 133.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Modify ROOT_DIR to your own directory containing imagesTr/labelsTr\n",
    "    images = sorted(glob(os.path.join(ROOT_DIR, \"imagesTr\", \"*.nii.gz\")))\n",
    "    labels = sorted(glob(os.path.join(ROOT_DIR, \"labelsTr\", \"*.nii.gz\")))\n",
    "\n",
    "    # Instantiate dataset & dataloader\n",
    "    train_dataset = HeartDataset(image_paths=images, label_paths=labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Create model\n",
    "    model = UNet3D(in_channels=1, out_channels=2, base_filters=32, bilinear=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer & loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- Mixed Precision Components ---\n",
    "    scaler = GradScaler()  # handles dynamic loss scaling for FP16\n",
    "\n",
    "    epochs = 2  # Example small number\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_loader):\n",
    "            # images shape: (B, 1, Z, Y, X)\n",
    "            # labels shape: (B, 1, Z, Y, X)\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # For nn.CrossEntropyLoss, labels should be shape (B, Z, Y, X)\n",
    "            labels = labels.squeeze(1)  # remove the channel dim => (B, Z, Y, X)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use autocast for forward + loss computation\n",
    "            with autocast():\n",
    "                outputs = model(images)  # shape: (B, 2, Z, Y, X)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backprop with scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % 5 == 0:\n",
    "                print(f\"[Epoch {epoch+1}, Step {step}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} finished. Average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_3d_slices(img_3d, title=\"\", figsize=(12, 4)):\n",
    "    \"\"\"\n",
    "    Show 3 slices from a 3D volume:\n",
    "      1) A slice in the axial plane (z fixed)\n",
    "      2) A slice in the coronal plane (y fixed)\n",
    "      3) A slice in the sagittal plane (x fixed)\n",
    "    img_3d: np.ndarray with shape (Z, Y, X)\n",
    "    title: optional string for figure title\n",
    "    \"\"\"\n",
    "    z, y, x = img_3d.shape\n",
    "\n",
    "    # pick the middle indices in each dimension\n",
    "    z_mid = z // 2\n",
    "    y_mid = y // 2\n",
    "    x_mid = x // 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "    # Axial plane: fix z index\n",
    "    axes[0].imshow(img_3d[z_mid, :, :], cmap=\"gray\")\n",
    "    axes[0].set_title(f\"Axial (z={z_mid})\")\n",
    "\n",
    "    # Coronal plane: fix y index\n",
    "    # coronal slice is shape [z, x], so we do img_3d[:, y_mid, :]\n",
    "    axes[1].imshow(img_3d[:, y_mid, :], cmap=\"gray\")\n",
    "    axes[1].set_title(f\"Coronal (y={y_mid})\")\n",
    "\n",
    "    # Sagittal plane: fix x index\n",
    "    # sagittal slice is shape [z, y], so we do img_3d[:, :, x_mid]\n",
    "    # We might transpose so that it doesn't appear rotated, but that's optional\n",
    "    axes[2].imshow(img_3d[:, :, x_mid].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[2].set_title(f\"Sagittal (x={x_mid})\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume you already have:\n",
    "#   dataset = HeartDataset(...)       # your custom dataset\n",
    "#   OR a DataLoader: loader = DataLoader(dataset, ...)\n",
    "# For simplicity, we'll just pull an item directly from the dataset.\n",
    "\n",
    "# 1) Get the first sample from the dataset\n",
    "img_tensor, lbl_tensor = dataset[0]  # shape: (1, Z, Y, X) each\n",
    "\n",
    "# 2) Convert to NumPy (removing the channel dimension)\n",
    "img_3d = img_tensor.squeeze(0).numpy()  # now shape = (Z, Y, X)\n",
    "\n",
    "# We'll pick the middle slice (axial view) along Z\n",
    "z_mid = img_3d.shape[0] // 2\n",
    "slice_2d = img_3d[z_mid]  # shape = (Y, X)\n",
    "\n",
    "# 3) Window/Level using percentile-based clipping to improve contrast\n",
    "p1, p99 = np.percentile(slice_2d, [1, 99])         # 1st & 99th percentile\n",
    "slice_clipped = np.clip(slice_2d, p1, p99)         # clamp intensities\n",
    "slice_normalized = (slice_clipped - p1) / (p99 - p1)  # scale to [0..1]\n",
    "\n",
    "# 4) Display this slice\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(slice_normalized, cmap='gray')\n",
    "plt.title(f\"Middle Slice (z={z_mid}) with Percentile Windowing\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# ------------ Utility functions ------------\n",
    "def load_nifti(path):\n",
    "    nifti = nib.load(path)\n",
    "    data = nifti.get_fdata(dtype=np.float32)\n",
    "    affine = nifti.affine\n",
    "    header = nifti.header\n",
    "    return data, affine, header\n",
    "\n",
    "def get_spacing_from_affine(affine):\n",
    "    sx = np.sqrt(affine[0, 0]**2 + affine[0, 1]**2 + affine[0, 2]**2)\n",
    "    sy = np.sqrt(affine[1, 0]**2 + affine[1, 1]**2 + affine[1, 2]**2)\n",
    "    sz = np.sqrt(affine[2, 0]**2 + affine[2, 1]**2 + affine[2, 2]**2)\n",
    "    return np.array([sz, sy, sx])\n",
    "\n",
    "def resample_volume(volume, current_spacing, new_spacing, is_label=False, order=1):\n",
    "    if is_label:\n",
    "        order = 0  # nearest-neighbor for labels\n",
    "    zoom_factors = current_spacing / new_spacing\n",
    "    return zoom(volume, zoom=zoom_factors, order=order)\n",
    "\n",
    "def min_max_scale_intensity(img, min_val=-57, max_val=164, clip=True):\n",
    "    \"\"\"\n",
    "    Scale intensity to [0, 1] range, assuming raw intensities\n",
    "    are roughly in [min_val, max_val]. Optionally clip to [0..1].\n",
    "    \"\"\"\n",
    "    img = (img - min_val) / (max_val - min_val)\n",
    "    if clip:\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "    return img\n",
    "\n",
    "# -------------- Dataset class --------------\n",
    "class HeartAugCompareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns both the pre-augmentation version and post-augmentation version.\n",
    "    For each sample, you get (img_pre, lbl_pre, img_aug, lbl_aug).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths,\n",
    "        label_paths,\n",
    "        target_spacing=np.array([1.25, 1.25, 1.25]),\n",
    "        do_augment=True\n",
    "    ):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.target_spacing = target_spacing\n",
    "        self.do_augment = do_augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Load image and label\n",
    "        img_path = self.image_paths[idx]\n",
    "        lbl_path = self.label_paths[idx]\n",
    "        img, aff_img, _ = load_nifti(img_path)\n",
    "        lbl, aff_lbl, _ = load_nifti(lbl_path)\n",
    "        \n",
    "        # 2) Resample\n",
    "        spacing_img = get_spacing_from_affine(aff_img)\n",
    "        spacing_lbl = get_spacing_from_affine(aff_lbl)\n",
    "        \n",
    "        img_rs = resample_volume(img, spacing_img, self.target_spacing, is_label=False)\n",
    "        lbl_rs = resample_volume(lbl, spacing_lbl, self.target_spacing, is_label=True)\n",
    "\n",
    "        # 3) Intensity normalization (for the image)\n",
    "        img_rs = img_rs.astype(np.float32)  # ensure float32\n",
    "        img_norm = min_max_scale_intensity(img_rs, min_val=-57, max_val=164, clip=True)\n",
    "        \n",
    "        # ---- Save a copy *before* augmentation ----\n",
    "        img_pre = img_norm.copy()\n",
    "        lbl_pre = lbl_rs.copy()\n",
    "\n",
    "        # 4) (Optional) augmentation\n",
    "        img_aug = img_norm\n",
    "        lbl_aug = lbl_rs\n",
    "\n",
    "        if self.do_augment:\n",
    "            # Random flip\n",
    "            if np.random.rand() < 0.5:\n",
    "                axis = np.random.choice([0,1,2])\n",
    "                img_aug = np.flip(img_aug, axis=axis).copy()\n",
    "                lbl_aug = np.flip(lbl_aug, axis=axis).copy()\n",
    "\n",
    "            # Random rotate 90\n",
    "            if np.random.rand() < 0.5:\n",
    "                k = np.random.randint(1, 4)\n",
    "                ax_pairs = [(0,1), (1,2), (0,2)]\n",
    "                axes = ax_pairs[np.random.randint(len(ax_pairs))]\n",
    "                img_aug = np.rot90(img_aug, k=k, axes=axes).copy()\n",
    "                lbl_aug = np.rot90(lbl_aug, k=k, axes=axes).copy()\n",
    "\n",
    "            # Random zoom\n",
    "            if np.random.rand() < 0.2:\n",
    "                zf = np.random.uniform(0.9, 1.1, size=3)\n",
    "                img_aug = zoom(img_aug, zoom=zf, order=1)\n",
    "                lbl_aug = zoom(lbl_aug, zoom=zf, order=0)\n",
    "\n",
    "        # 5) Convert everything to torch tensors\n",
    "        # shape (1, Z, Y, X)\n",
    "        img_pre_tensor = torch.from_numpy(img_pre).unsqueeze(0).float()\n",
    "        lbl_pre_tensor = torch.from_numpy(lbl_pre).unsqueeze(0).long()\n",
    "\n",
    "        img_aug_tensor = torch.from_numpy(img_aug).unsqueeze(0).float()\n",
    "        lbl_aug_tensor = torch.from_numpy(lbl_aug).unsqueeze(0).long()\n",
    "\n",
    "        # Return the \"before\" + \"after\" versions\n",
    "        return (img_pre_tensor, lbl_pre_tensor, img_aug_tensor, lbl_aug_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/home/raghuram/ARPL/MR-Image-Reconstruction-Using-Deep-Learning/Task02_Heart\"\n",
    "images = sorted(glob(root_dir + \"/imagesTr/*.nii.gz\"))\n",
    "labels = sorted(glob(root_dir + \"/labelsTr/*.nii.gz\"))\n",
    "\n",
    "# Create the dataset\n",
    "dataset = HeartAugCompareDataset(\n",
    "    image_paths=images,\n",
    "    label_paths=labels,\n",
    "    target_spacing=np.array([1.25, 1.25, 1.25]),\n",
    "    do_augment=True  # set True if you want random flips/rotations\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Fetch one batch\n",
    "img_pre, lbl_pre, img_aug, lbl_aug = next(iter(loader))\n",
    "# shapes: (1, 1, Z, Y, X) each\n",
    "\n",
    "# Convert to numpy, remove the batch & channel dimension\n",
    "img_pre_3d = img_pre[0,0].cpu().numpy()  # shape (Z, Y, X)\n",
    "img_aug_3d = img_aug[0,0].cpu().numpy()  # shape (Z, Y, X)\n",
    "\n",
    "# Let's pick the middle slice along z\n",
    "z_mid = img_pre_3d.shape[0] // 2\n",
    "slice_pre = img_pre_3d[z_mid]\n",
    "slice_aug = img_aug_3d[z_mid]\n",
    "\n",
    "# Plot side-by-side\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(slice_pre, cmap='gray')\n",
    "plt.title(\"Before Augmentation\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(slice_aug, cmap='gray')\n",
    "plt.title(\"After Augmentation\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
