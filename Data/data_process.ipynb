{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import zoom, rotate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/raghuram/ARPL/MR-Image-Reconstruction-Using-Deep-Learning/Task02_Heart\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(path):\n",
    "    \"\"\"\n",
    "    Load a .nii.gz file using nibabel and return:\n",
    "      - data (as a np.ndarray)\n",
    "      - affine (4x4 transform matrix)\n",
    "      - header (NIfTI header)\n",
    "    \"\"\"\n",
    "    nifti = nib.load(path)\n",
    "    data = nifti.get_fdata(dtype=np.float32)  # or np.float64 if you prefer\n",
    "    affine = nifti.affine\n",
    "    header = nifti.header\n",
    "    return data, affine, header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacing_from_affine(affine):\n",
    "    \"\"\"\n",
    "    Given a 4x4 affine matrix from nibabel, extract voxel spacing\n",
    "    along each dimension: (z, y, x).\n",
    "    \"\"\"\n",
    "    # Typically, the diagonal or row vectors of the affine (in absolute value)\n",
    "    # correspond to voxel size. A simple approach:\n",
    "    sx = np.sqrt(affine[0, 0]**2 + affine[0, 1]**2 + affine[0, 2]**2)\n",
    "    sy = np.sqrt(affine[1, 0]**2 + affine[1, 1]**2 + affine[1, 2]**2)\n",
    "    sz = np.sqrt(affine[2, 0]**2 + affine[2, 1]**2 + affine[2, 2]**2)\n",
    "    return np.array([sz, sy, sx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 4\n",
    "\n",
    "def get_spacing_from_affine(affine):\n",
    "    \"\"\"\n",
    "    Given a 4x4 affine matrix from nibabel, extract the voxel spacing\n",
    "    along each dimension (z, y, x).\n",
    "    \"\"\"\n",
    "    sx = np.sqrt(affine[0, 0]**2 + affine[0, 1]**2 + affine[0, 2]**2)\n",
    "    sy = np.sqrt(affine[1, 0]**2 + affine[1, 1]**2 + affine[1, 2]**2)\n",
    "    sz = np.sqrt(affine[2, 0]**2 + affine[2, 1]**2 + affine[2, 2]**2)\n",
    "    return np.array([sz, sy, sx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 5\n",
    "\n",
    "def resample_volume(volume, current_spacing, new_spacing, is_label=False, order=1):\n",
    "    \"\"\"\n",
    "    Resample a 3D volume (or label) to new_spacing using scipy.ndimage.zoom.\n",
    "      - volume: np.ndarray of shape (z, y, x).\n",
    "      - current_spacing: array-like, e.g. [sz, sy, sx].\n",
    "      - new_spacing: array-like, e.g. [nz, ny, nx].\n",
    "      - is_label: If True, use nearest neighbor for labels (order=0).\n",
    "      - order: interpolation order for images (1=linear by default).\n",
    "\n",
    "    Returns the resampled volume as np.ndarray.\n",
    "    \"\"\"\n",
    "    if is_label:\n",
    "        order = 0  # nearest-neighbor for segmentations\n",
    "\n",
    "    zoom_factors = current_spacing / new_spacing\n",
    "    resampled = zoom(volume, zoom=zoom_factors, order=order)\n",
    "    return resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 6\n",
    "\n",
    "def min_max_scale_intensity(img, min_val=-57, max_val=164, clip=True):\n",
    "    \"\"\"\n",
    "    Scale intensity to [0, 1] range, assuming raw intensities\n",
    "    lie roughly in [min_val, max_val]. Optionally clip to [0,1].\n",
    "    \"\"\"\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - min_val) / (max_val - min_val)\n",
    "    if clip:\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 8\n",
    "\n",
    "def random_rotate90_3d(img, prob=0.5, max_k=3):\n",
    "    \"\"\"\n",
    "    Randomly rotate the 3D image by 90, 180, or 270 degrees (k=1..max_k)\n",
    "    around a random plane of axes, with probability prob.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < prob:\n",
    "        k = np.random.randint(1, max_k + 1)  # 1, 2, or 3\n",
    "        axis_pairs = [(0,1), (1,2), (0,2)]    # e.g., rotate in XY, YZ, or XZ plane\n",
    "        axes = axis_pairs[np.random.randint(len(axis_pairs))]\n",
    "        img = np.rot90(img, k=k, axes=axes)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Cell 9\n",
    "\n",
    "def random_zoom_3d(img, prob=0.2, min_zoom=0.9, max_zoom=1.1, order=1, is_label=False):\n",
    "    \"\"\"\n",
    "    Randomly zoom a 3D volume in/out with a factor in [min_zoom, max_zoom],\n",
    "    with probability prob. If is_label=True, uses nearest neighbor.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < prob:\n",
    "        zf = np.random.uniform(min_zoom, max_zoom, size=3)\n",
    "        if is_label:\n",
    "            order = 0  # nearest for labels\n",
    "        img = zoom(img, zoom=zf, order=order)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 20/20 [00:21<00:00,  1.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>shape_img</th>\n",
       "      <th>shape_lbl</th>\n",
       "      <th>img_min</th>\n",
       "      <th>img_max</th>\n",
       "      <th>lbl_min</th>\n",
       "      <th>lbl_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 130)</td>\n",
       "      <td>(351, 320, 130)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 110)</td>\n",
       "      <td>(351, 320, 110)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(337, 291, 112)</td>\n",
       "      <td>(337, 291, 112)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(362, 325, 133)</td>\n",
       "      <td>(362, 325, 133)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 100)</td>\n",
       "      <td>(351, 320, 100)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 120, 320)</td>\n",
       "      <td>(351, 120, 320)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 90)</td>\n",
       "      <td>(351, 320, 90)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>/home/raghuram/ARPL/MR-Image-Reconstruction-Us...</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>(351, 320, 120)</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         image_path  \\\n",
       "0      0  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "1      1  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "2      2  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "3      3  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "4      4  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "5      5  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "6      6  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "7      7  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "8      8  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "9      9  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   \n",
       "\n",
       "                                          label_path        shape_img  \\\n",
       "0  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 130)   \n",
       "1  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 110)   \n",
       "2  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (337, 291, 112)   \n",
       "3  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (362, 325, 133)   \n",
       "4  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 100)   \n",
       "5  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 120)   \n",
       "6  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 120, 320)   \n",
       "7  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 120)   \n",
       "8  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...   (351, 320, 90)   \n",
       "9  /home/raghuram/ARPL/MR-Image-Reconstruction-Us...  (351, 320, 120)   \n",
       "\n",
       "         shape_lbl   img_min  img_max  lbl_min  lbl_max  \n",
       "0  (351, 320, 130)  0.257919      1.0      0.0      1.0  \n",
       "1  (351, 320, 110)  0.257919      1.0      0.0      1.0  \n",
       "2  (337, 291, 112)  0.257919      1.0      0.0      1.0  \n",
       "3  (362, 325, 133)  0.257919      1.0      0.0      1.0  \n",
       "4  (351, 320, 100)  0.257919      1.0      0.0      1.0  \n",
       "5  (351, 320, 120)  0.257919      1.0      0.0      1.0  \n",
       "6  (351, 120, 320)  0.257919      1.0      0.0      1.0  \n",
       "7  (351, 320, 120)  0.257919      1.0      0.0      1.0  \n",
       "8   (351, 320, 90)  0.257919      1.0      0.0      1.0  \n",
       "9  (351, 320, 120)  0.257919      1.0      0.0      1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jupyter Cell 10\n",
    "\n",
    "# 1) Gather file paths\n",
    "images = sorted(glob(os.path.join(ROOT_DIR, \"imagesTr\", \"*.nii.gz\")))\n",
    "labels = sorted(glob(os.path.join(ROOT_DIR, \"labelsTr\", \"*.nii.gz\")))\n",
    "\n",
    "# 2) Set a desired target spacing (z, y, x)\n",
    "target_spacing = np.array([1.25, 1.25, 1.25])\n",
    "\n",
    "# 3) Prepare a list for stats\n",
    "stats_list = []\n",
    "\n",
    "# 4) Loop over all images/labels\n",
    "for idx, (img_path, lbl_path) in enumerate(tqdm(zip(images, labels), \n",
    "                                                total=len(images),\n",
    "                                                desc=\"Processing\")):\n",
    "    # --- Load\n",
    "    img, aff_img, _ = load_nifti(img_path)\n",
    "    lbl, aff_lbl, _ = load_nifti(lbl_path)\n",
    "    \n",
    "    # --- Original spacings\n",
    "    orig_spacing_img = get_spacing_from_affine(aff_img)\n",
    "    orig_spacing_lbl = get_spacing_from_affine(aff_lbl)  # often the same as img\n",
    "    \n",
    "    # --- Resample\n",
    "    img_rs = resample_volume(img, orig_spacing_img, target_spacing, is_label=False, order=1)\n",
    "    lbl_rs = resample_volume(lbl, orig_spacing_lbl, target_spacing, is_label=True)\n",
    "    \n",
    "    # --- Intensity normalization (image only)\n",
    "    img_scaled = min_max_scale_intensity(img_rs, min_val=-57, max_val=164, clip=True)\n",
    "    \n",
    "    # --- Data augmentation\n",
    "    # Must apply the same random transforms to both image & label\n",
    "\n",
    "    # Random flip\n",
    "    if np.random.rand() < 0.5:\n",
    "        flip_axis = np.random.choice([0,1,2])\n",
    "        img_scaled = np.flip(img_scaled, axis=flip_axis)\n",
    "        lbl_rs = np.flip(lbl_rs, axis=flip_axis)\n",
    "\n",
    "    # Random rotate 90\n",
    "    if np.random.rand() < 0.5:\n",
    "        k = np.random.randint(1, 4)  # 1, 2, or 3\n",
    "        axis_pairs = [(0,1), (1,2), (0,2)]\n",
    "        ax = axis_pairs[np.random.randint(len(axis_pairs))]\n",
    "        img_scaled = np.rot90(img_scaled, k=k, axes=ax)\n",
    "        lbl_rs = np.rot90(lbl_rs, k=k, axes=ax)\n",
    "\n",
    "    # Random zoom\n",
    "    if np.random.rand() < 0.2:\n",
    "        zf = np.random.uniform(0.9, 1.1, size=3)\n",
    "        img_scaled = zoom(img_scaled, zoom=zf, order=1)\n",
    "        lbl_rs = zoom(lbl_rs, zoom=zf, order=0)\n",
    "\n",
    "    # --- Collect stats\n",
    "    img_min, img_max = float(img_scaled.min()), float(img_scaled.max())\n",
    "    lbl_min, lbl_max = float(lbl_rs.min()), float(lbl_rs.max())\n",
    "\n",
    "    stats_list.append({\n",
    "        \"index\": idx,\n",
    "        \"image_path\": img_path,\n",
    "        \"label_path\": lbl_path,\n",
    "        \"shape_img\": img_scaled.shape,\n",
    "        \"shape_lbl\": lbl_rs.shape,\n",
    "        \"img_min\": img_min,\n",
    "        \"img_max\": img_max,\n",
    "        \"lbl_min\": lbl_min,\n",
    "        \"lbl_max\": lbl_max\n",
    "    })\n",
    "\n",
    "# 5) Convert stats to a DataFrame\n",
    "df_stats = pd.DataFrame(stats_list)\n",
    "df_stats.to_csv(\"heart_dataset_stats.csv\", index=False)\n",
    "df_stats.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def center_crop_3d(volume, crop_shape):\n",
    "    \"\"\"\n",
    "    Center-crop a 3D volume (Z, Y, X) to the desired crop_shape.\n",
    "    If the volume is smaller than crop_shape along any dimension,\n",
    "    it will just return the original volume for that dimension.\n",
    "    \"\"\"\n",
    "    z, y, x = volume.shape\n",
    "    cz, cy, cx = crop_shape\n",
    "\n",
    "    start_z = max((z - cz) // 2, 0)\n",
    "    start_y = max((y - cy) // 2, 0)\n",
    "    start_x = max((x - cx) // 2, 0)\n",
    "\n",
    "    end_z = start_z + cz if (start_z + cz) <= z else z\n",
    "    end_y = start_y + cy if (start_y + cy) <= y else y\n",
    "    end_x = start_x + cx if (start_x + cx) <= x else x\n",
    "\n",
    "    return volume[start_z:end_z, start_y:end_y, start_x:end_x]\n",
    "\n",
    "\n",
    "class ModifiedHeartDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 3D NIfTI images and labels, applies a center-crop to reduce size\n",
    "    (and thus memory usage), and returns them as torch tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, label_paths, crop_shape=(128, 128, 128)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (list): list of paths to image .nii(.gz) files.\n",
    "            label_paths (list): list of paths to label .nii(.gz) files.\n",
    "            crop_shape  (tuple): desired 3D shape after center-cropping (Z, Y, X).\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.crop_shape = crop_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        lbl_path = self.label_paths[idx]\n",
    "\n",
    "        # Load volumes (shape: [Z, Y, X])\n",
    "        img_nifti = nib.load(img_path)\n",
    "        lbl_nifti = nib.load(lbl_path)\n",
    "\n",
    "        img = img_nifti.get_fdata(dtype=np.float32)\n",
    "        lbl = lbl_nifti.get_fdata(dtype=np.float32)\n",
    "\n",
    "        # Center-crop\n",
    "        img_cropped = center_crop_3d(img, self.crop_shape)\n",
    "        lbl_cropped = center_crop_3d(lbl, self.crop_shape)\n",
    "\n",
    "        # Expand dims to (1, Z, Y, X)\n",
    "        img_cropped = np.expand_dims(img_cropped, axis=0)\n",
    "        lbl_cropped = np.expand_dims(lbl_cropped, axis=0)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        img_tensor = torch.from_numpy(img_cropped)\n",
    "        lbl_tensor = torch.from_numpy(lbl_cropped)\n",
    "\n",
    "        return img_tensor, lbl_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataset\n",
    "# dataset = HeartDataset(\n",
    "#     image_paths=images,\n",
    "#     label_paths=labels,\n",
    "#     target_spacing=np.array([1.25, 1.25, 1.25]),)\n",
    "\n",
    "# # Create DataLoader\n",
    "# loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper module that performs a 3D convolution -> ReLU -> 3D convolution -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Downscaling (via MaxPool3D) followed by a DoubleConv3D.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling then a DoubleConv3D. We can do either:\n",
    "      - Transposed Conv if bilinear=False\n",
    "      - nn.Upsample (trilinear) if bilinear=True\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "        # If using bilinear upsampling, keep #params lower:\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is decoder feature, x2 is skip connection from encoder\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Match sizes by padding\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2])\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3D U-Net with 4 down-sampling / up-sampling levels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=32, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv3D(in_channels, base_filters)\n",
    "        self.down1 = Down3D(base_filters, base_filters * 2)\n",
    "        self.down2 = Down3D(base_filters * 2, base_filters * 4)\n",
    "        self.down3 = Down3D(base_filters * 4, base_filters * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down3D(base_filters * 8, base_filters * 16 // factor)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = Up3D(base_filters * 16, base_filters * 8 // factor, bilinear)\n",
    "        self.up2 = Up3D(base_filters * 8, base_filters * 4 // factor, bilinear)\n",
    "        self.up3 = Up3D(base_filters * 4, base_filters * 2 // factor, bilinear)\n",
    "        self.up4 = Up3D(base_filters * 2, base_filters, bilinear)\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        self.outc = nn.Conv3d(base_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop_3d(img, lbl, crop_size=(64, 128, 128)):\n",
    "    \"\"\"\n",
    "    Randomly crop a 3D patch of size crop_size from img and lbl.\n",
    "    Assume img, lbl shape: (Z, Y, X).\n",
    "    \"\"\"\n",
    "    z, y, x = img.shape\n",
    "    cz, cy, cx = crop_size\n",
    "    \n",
    "    # pick random start\n",
    "    z0 = np.random.randint(0, z - cz) if z > cz else 0\n",
    "    y0 = np.random.randint(0, y - cy) if y > cy else 0\n",
    "    x0 = np.random.randint(0, x - cx) if x > cx else 0\n",
    "    \n",
    "    img_patch = img[z0:z0+cz, y0:y0+cy, x0:x0+cx]\n",
    "    lbl_patch = lbl[z0:z0+cz, y0:y0+cy, x0:x0+cx]\n",
    "    return img_patch, lbl_patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 finished. Average loss: 0.7787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 finished. Average loss: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 finished. Average loss: 0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 finished. Average loss: 0.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 finished. Average loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 finished. Average loss: 0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 finished. Average loss: 0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 finished. Average loss: 0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 finished. Average loss: 0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 finished. Average loss: 0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 finished. Average loss: 0.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 finished. Average loss: 0.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 finished. Average loss: 0.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 finished. Average loss: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 finished. Average loss: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 finished. Average loss: 0.0355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 finished. Average loss: 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 finished. Average loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 finished. Average loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 finished. Average loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 finished. Average loss: 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 finished. Average loss: 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 finished. Average loss: 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 finished. Average loss: 0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 finished. Average loss: 0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 finished. Average loss: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 finished. Average loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 finished. Average loss: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 finished. Average loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 finished. Average loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 finished. Average loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 finished. Average loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 finished. Average loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 finished. Average loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 finished. Average loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 finished. Average loss: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 finished. Average loss: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 finished. Average loss: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 finished. Average loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 finished. Average loss: 0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 finished. Average loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 finished. Average loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 finished. Average loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 finished. Average loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 finished. Average loss: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 finished. Average loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 finished. Average loss: 0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 finished. Average loss: 0.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 finished. Average loss: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 finished. Average loss: 0.0084\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 finished. Average loss: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 finished. Average loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 finished. Average loss: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 finished. Average loss: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 finished. Average loss: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 finished. Average loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 finished. Average loss: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 finished. Average loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 finished. Average loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 finished. Average loss: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 finished. Average loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 finished. Average loss: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 finished. Average loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 finished. Average loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 finished. Average loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 finished. Average loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 finished. Average loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 finished. Average loss: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 finished. Average loss: 0.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 finished. Average loss: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 finished. Average loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 finished. Average loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 finished. Average loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 finished. Average loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 finished. Average loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 finished. Average loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 finished. Average loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 finished. Average loss: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 finished. Average loss: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 finished. Average loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 finished. Average loss: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 finished. Average loss: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 finished. Average loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 finished. Average loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 finished. Average loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 finished. Average loss: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 finished. Average loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 finished. Average loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 finished. Average loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 finished. Average loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 finished. Average loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 finished. Average loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 finished. Average loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 finished. Average loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 finished. Average loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 finished. Average loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 finished. Average loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 finished. Average loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 finished. Average loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 finished. Average loss: 0.0056\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    # Example root directory containing imagesTr/labelsTr\n",
    "    images = sorted(glob(os.path.join(ROOT_DIR, \"imagesTr\", \"*.nii.gz\")))\n",
    "    labels = sorted(glob(os.path.join(ROOT_DIR, \"labelsTr\", \"*.nii.gz\")))\n",
    "\n",
    "    # 1) Instantiate dataset & data loader\n",
    "    train_dataset = ModifiedHeartDataset(\n",
    "        image_paths=images,\n",
    "        label_paths=labels,\n",
    "        crop_shape=(128, 128, 128)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 2) Create the 3D UNet model\n",
    "    model = UNet3D(in_channels=1, out_channels=2, base_filters=32, bilinear=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 3) Define optimizer & loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 4) Mixed-precision (AMP) setup\n",
    "    #    - Do NOT pass \"cuda\" as a positional arg. Just use GradScaler() or specify enabled=True.\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # (Optional) directory for checkpoints\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    # 5) Training loop (example: 100 epochs)\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "        for step, (images_batch, labels_batch) in enumerate(pbar):\n",
    "            images_batch = images_batch.to(device, dtype=torch.float32)\n",
    "            labels_batch = labels_batch.to(device, dtype=torch.long)\n",
    "            labels_batch = labels_batch.squeeze(1)  # shape: (B, Z, Y, X)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use the NEW recommended autocast signature:\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(images_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            # Scale gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if step % 5 == 0:\n",
    "                pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} finished. Average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoints every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            ckpt_path = os.path.join(\"checkpoints\", f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"Checkpoint saved to {ckpt_path}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_3d_slices(img_3d, title=\"\", figsize=(12, 4)):\n",
    "    \"\"\"\n",
    "    Show 3 slices from a 3D volume:\n",
    "      1) A slice in the axial plane (z fixed)\n",
    "      2) A slice in the coronal plane (y fixed)\n",
    "      3) A slice in the sagittal plane (x fixed)\n",
    "    img_3d: np.ndarray with shape (Z, Y, X)\n",
    "    title: optional string for figure title\n",
    "    \"\"\"\n",
    "    z, y, x = img_3d.shape\n",
    "\n",
    "    # pick the middle indices in each dimension\n",
    "    z_mid = z // 2\n",
    "    y_mid = y // 2\n",
    "    x_mid = x // 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "    # Axial plane: fix z index\n",
    "    axes[0].imshow(img_3d[z_mid, :, :], cmap=\"gray\")\n",
    "    axes[0].set_title(f\"Axial (z={z_mid})\")\n",
    "\n",
    "    # Coronal plane: fix y index\n",
    "    # coronal slice is shape [z, x], so we do img_3d[:, y_mid, :]\n",
    "    axes[1].imshow(img_3d[:, y_mid, :], cmap=\"gray\")\n",
    "    axes[1].set_title(f\"Coronal (y={y_mid})\")\n",
    "\n",
    "    # Sagittal plane: fix x index\n",
    "    # sagittal slice is shape [z, y], so we do img_3d[:, :, x_mid]\n",
    "    # We might transpose so that it doesn't appear rotated, but that's optional\n",
    "    axes[2].imshow(img_3d[:, :, x_mid].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[2].set_title(f\"Sagittal (x={x_mid})\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume you already have:\n",
    "#   dataset = HeartDataset(...)       # your custom dataset\n",
    "#   OR a DataLoader: loader = DataLoader(dataset, ...)\n",
    "# For simplicity, we'll just pull an item directly from the dataset.\n",
    "\n",
    "# 1) Get the first sample from the dataset\n",
    "img_tensor, lbl_tensor = dataset[0]  # shape: (1, Z, Y, X) each\n",
    "\n",
    "# 2) Convert to NumPy (removing the channel dimension)\n",
    "img_3d = img_tensor.squeeze(0).numpy()  # now shape = (Z, Y, X)\n",
    "\n",
    "# We'll pick the middle slice (axial view) along Z\n",
    "z_mid = img_3d.shape[0] // 2\n",
    "slice_2d = img_3d[z_mid]  # shape = (Y, X)\n",
    "\n",
    "# 3) Window/Level using percentile-based clipping to improve contrast\n",
    "p1, p99 = np.percentile(slice_2d, [1, 99])         # 1st & 99th percentile\n",
    "slice_clipped = np.clip(slice_2d, p1, p99)         # clamp intensities\n",
    "slice_normalized = (slice_clipped - p1) / (p99 - p1)  # scale to [0..1]\n",
    "\n",
    "# 4) Display this slice\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(slice_normalized, cmap='gray')\n",
    "plt.title(f\"Middle Slice (z={z_mid}) with Percentile Windowing\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.ndimage import zoom\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm  # Progress tracking\n",
    "\n",
    "class SlicesDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths=None, axis=0, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.axis = axis\n",
    "        self.transform = transform\n",
    "        self.slice_index_mapping = []  # Stores (file_idx, slice_idx)\n",
    "\n",
    "        print(\"Indexing slices from NIfTI files...\")\n",
    "        \n",
    "        # Create index mapping without loading files into memory\n",
    "        for file_idx, img_path in enumerate(tqdm(self.image_paths, desc=\"Indexing\", unit=\"file\")):\n",
    "            img_nifti = nib.load(img_path)\n",
    "            img_shape = img_nifti.shape  # Get shape without loading into memory\n",
    "            num_slices = img_shape[self.axis]\n",
    "\n",
    "            for slice_idx in range(num_slices):\n",
    "                self.slice_index_mapping.append((file_idx, slice_idx))  # Store file & slice index\n",
    "\n",
    "        print(f\"Total slices indexed: {len(self.slice_index_mapping)}\")\n",
    "\n",
    "        # Optional: Cache the last loaded file to avoid redundant loading\n",
    "        self.last_loaded_file = None\n",
    "        self.last_loaded_data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_index_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, slice_idx = self.slice_index_mapping[idx]\n",
    "        img_path = self.image_paths[file_idx]\n",
    "        label_path = self.label_paths[file_idx] if self.label_paths else None\n",
    "\n",
    "        # Load image if it's not already cached\n",
    "        if self.last_loaded_file != img_path:\n",
    "            img_nifti = nib.load(img_path)\n",
    "            self.last_loaded_data = img_nifti.get_fdata(dtype=np.float32)\n",
    "            self.last_loaded_file = img_path  # Update cache\n",
    "\n",
    "        img_3d = self.last_loaded_data\n",
    "        img_slice = img_3d.take(slice_idx, axis=self.axis)\n",
    "\n",
    "        # Normalize image\n",
    "        img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())\n",
    "\n",
    "        # Load label (if available)\n",
    "        label_slice = None\n",
    "        if label_path:\n",
    "            if self.last_loaded_file != label_path:\n",
    "                label_nifti = nib.load(label_path)\n",
    "                label_3d = label_nifti.get_fdata(dtype=np.float32).astype(np.int32)\n",
    "                self.last_loaded_data = label_3d\n",
    "                self.last_loaded_file = label_path  # Update cache\n",
    "            label_slice = self.last_loaded_data.take(slice_idx, axis=self.axis)\n",
    "\n",
    "        # Convert to tensors\n",
    "        img_tensor = torch.from_numpy(img_slice[np.newaxis, ...]).float()\n",
    "        label_tensor = torch.from_numpy(label_slice).long() if label_slice is not None else None\n",
    "\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "# ------------ Utility functions ------------\n",
    "def load_nifti(path):\n",
    "    nifti = nib.load(path)\n",
    "    data = nifti.get_fdata(dtype=np.float32)\n",
    "    affine = nifti.affine\n",
    "    header = nifti.header\n",
    "    return data, affine, header\n",
    "\n",
    "def get_spacing_from_affine(affine):\n",
    "    sx = np.sqrt(affine[0, 0]**2 + affine[0, 1]**2 + affine[0, 2]**2)\n",
    "    sy = np.sqrt(affine[1, 0]**2 + affine[1, 1]**2 + affine[1, 2]**2)\n",
    "    sz = np.sqrt(affine[2, 0]**2 + affine[2, 1]**2 + affine[2, 2]**2)\n",
    "    return np.array([sz, sy, sx])\n",
    "\n",
    "def resample_volume(volume, current_spacing, new_spacing, is_label=False, order=1):\n",
    "    if is_label:\n",
    "        order = 0  # nearest-neighbor for labels\n",
    "    zoom_factors = current_spacing / new_spacing\n",
    "    return zoom(volume, zoom=zoom_factors, order=order)\n",
    "\n",
    "def min_max_scale_intensity(img, min_val=-57, max_val=164, clip=True):\n",
    "    \"\"\"\n",
    "    Scale intensity to [0, 1] range, assuming raw intensities\n",
    "    are roughly in [min_val, max_val]. Optionally clip to [0..1].\n",
    "    \"\"\"\n",
    "    img = (img - min_val) / (max_val - min_val)\n",
    "    if clip:\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "    return img\n",
    "\n",
    "# -------------- Dataset class --------------\n",
    "class HeartAugCompareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns both the pre-augmentation version and post-augmentation version.\n",
    "    For each sample, you get (img_pre, lbl_pre, img_aug, lbl_aug).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths,\n",
    "        label_paths,\n",
    "        target_spacing=np.array([1.25, 1.25, 1.25]),\n",
    "        do_augment=True\n",
    "    ):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.target_spacing = target_spacing\n",
    "        self.do_augment = do_augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Load image and label\n",
    "        img_path = self.image_paths[idx]\n",
    "        lbl_path = self.label_paths[idx]\n",
    "        img, aff_img, _ = load_nifti(img_path)\n",
    "        lbl, aff_lbl, _ = load_nifti(lbl_path)\n",
    "        \n",
    "        # 2) Resample\n",
    "        spacing_img = get_spacing_from_affine(aff_img)\n",
    "        spacing_lbl = get_spacing_from_affine(aff_lbl)\n",
    "        \n",
    "        img_rs = resample_volume(img, spacing_img, self.target_spacing, is_label=False)\n",
    "        lbl_rs = resample_volume(lbl, spacing_lbl, self.target_spacing, is_label=True)\n",
    "\n",
    "        # 3) Intensity normalization (for the image)\n",
    "        img_rs = img_rs.astype(np.float32)  # ensure float32\n",
    "        img_norm = min_max_scale_intensity(img_rs, min_val=-57, max_val=164, clip=True)\n",
    "        \n",
    "        # ---- Save a copy *before* augmentation ----\n",
    "        img_pre = img_norm.copy()\n",
    "        lbl_pre = lbl_rs.copy()\n",
    "\n",
    "        # 4) (Optional) augmentation\n",
    "        img_aug = img_norm\n",
    "        lbl_aug = lbl_rs\n",
    "\n",
    "        if self.do_augment:\n",
    "            # Random flip\n",
    "            if np.random.rand() < 0.5:\n",
    "                axis = np.random.choice([0,1,2])\n",
    "                img_aug = np.flip(img_aug, axis=axis).copy()\n",
    "                lbl_aug = np.flip(lbl_aug, axis=axis).copy()\n",
    "\n",
    "            # Random rotate 90\n",
    "            if np.random.rand() < 0.5:\n",
    "                k = np.random.randint(1, 4)\n",
    "                ax_pairs = [(0,1), (1,2), (0,2)]\n",
    "                axes = ax_pairs[np.random.randint(len(ax_pairs))]\n",
    "                img_aug = np.rot90(img_aug, k=k, axes=axes).copy()\n",
    "                lbl_aug = np.rot90(lbl_aug, k=k, axes=axes).copy()\n",
    "\n",
    "            # Random zoom\n",
    "            if np.random.rand() < 0.2:\n",
    "                zf = np.random.uniform(0.9, 1.1, size=3)\n",
    "                img_aug = zoom(img_aug, zoom=zf, order=1)\n",
    "                lbl_aug = zoom(lbl_aug, zoom=zf, order=0)\n",
    "\n",
    "        # 5) Convert everything to torch tensors\n",
    "        # shape (1, Z, Y, X)\n",
    "        img_pre_tensor = torch.from_numpy(img_pre).unsqueeze(0).float()\n",
    "        lbl_pre_tensor = torch.from_numpy(lbl_pre).unsqueeze(0).long()\n",
    "\n",
    "        img_aug_tensor = torch.from_numpy(img_aug).unsqueeze(0).float()\n",
    "        lbl_aug_tensor = torch.from_numpy(lbl_aug).unsqueeze(0).long()\n",
    "\n",
    "        # Return the \"before\" + \"after\" versions\n",
    "        return (img_pre_tensor, lbl_pre_tensor, img_aug_tensor, lbl_aug_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"/home/raghuram/ARPL/MR-Image-Reconstruction-Using-Deep-Learning/Task02_Heart\"\n",
    "images = sorted(glob(root_dir + \"/imagesTr/*.nii.gz\"))\n",
    "labels = sorted(glob(root_dir + \"/labelsTr/*.nii.gz\"))\n",
    "\n",
    "# Create the dataset\n",
    "dataset = HeartAugCompareDataset(\n",
    "    image_paths=images,\n",
    "    label_paths=labels,\n",
    "    target_spacing=np.array([1.25, 1.25, 1.25]),\n",
    "    do_augment=True  # set True if you want random flips/rotations\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Fetch one batch\n",
    "img_pre, lbl_pre, img_aug, lbl_aug = next(iter(loader))\n",
    "# shapes: (1, 1, Z, Y, X) each\n",
    "\n",
    "# Convert to numpy, remove the batch & channel dimension\n",
    "img_pre_3d = img_pre[0,0].cpu().numpy()  # shape (Z, Y, X)\n",
    "img_aug_3d = img_aug[0,0].cpu().numpy()  # shape (Z, Y, X)\n",
    "\n",
    "# Let's pick the middle slice along z\n",
    "z_mid = img_pre_3d.shape[0] // 2\n",
    "slice_pre = img_pre_3d[z_mid]\n",
    "slice_aug = img_aug_3d[z_mid]\n",
    "\n",
    "# Plot side-by-side\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(slice_pre, cmap='gray')\n",
    "plt.title(\"Before Augmentation\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(slice_aug, cmap='gray')\n",
    "plt.title(\"After Augmentation\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.py\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def center_crop_3d(volume, crop_shape):\n",
    "    z, y, x = volume.shape\n",
    "    cz, cy, cx = crop_shape\n",
    "\n",
    "    start_z = max((z - cz) // 2, 0)\n",
    "    start_y = max((y - cy) // 2, 0)\n",
    "    start_x = max((x - cx) // 2, 0)\n",
    "\n",
    "    end_z = min(start_z + cz, z)\n",
    "    end_y = min(start_y + cy, y)\n",
    "    end_x = min(start_x + cx, x)\n",
    "\n",
    "    return volume[start_z:end_z, start_y:end_y, start_x:end_x]\n",
    "\n",
    "\n",
    "class TestOnlyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads 3D NIfTI images, optionally center-crops them,\n",
    "    and returns them as PyTorch tensors (no labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, crop_shape=(128, 128, 128)):\n",
    "        self.image_paths = image_paths\n",
    "        self.crop_shape = crop_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img_nifti = nib.load(img_path)\n",
    "        img = img_nifti.get_fdata(dtype=np.float32)  # (Z, Y, X)\n",
    "\n",
    "        # center-crop (if needed)\n",
    "        if self.crop_shape is not None:\n",
    "            img = center_crop_3d(img, self.crop_shape)\n",
    "\n",
    "        # expand dims => (1, Z, Y, X)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        img_tensor = torch.from_numpy(img)  # shape (1, Z, Y, X)\n",
    "        return img_tensor, img_path  # We return the image path too for naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36877/2003229611.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing la_001.nii.gz with patch-based inference...\n",
      "Finished processing la_001.nii.gz\n",
      "Processing la_002.nii.gz with patch-based inference...\n",
      "Finished processing la_002.nii.gz\n",
      "Processing la_006.nii.gz with patch-based inference...\n",
      "Finished processing la_006.nii.gz\n",
      "Processing la_008.nii.gz with patch-based inference...\n",
      "Finished processing la_008.nii.gz\n",
      "Processing la_012.nii.gz with patch-based inference...\n",
      "Finished processing la_012.nii.gz\n",
      "Processing la_013.nii.gz with patch-based inference...\n",
      "Finished processing la_013.nii.gz\n",
      "Processing la_015.nii.gz with patch-based inference...\n",
      "Finished processing la_015.nii.gz\n",
      "Processing la_025.nii.gz with patch-based inference...\n",
      "Finished processing la_025.nii.gz\n",
      "Processing la_027.nii.gz with patch-based inference...\n",
      "Finished processing la_027.nii.gz\n",
      "Processing la_028.nii.gz with patch-based inference...\n",
      "Finished processing la_028.nii.gz\n",
      "Inference results saved to: inference_results.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Sliding-window Inference Function\n",
    "# =============================================================================\n",
    "def sliding_window_inference(volume, model, patch_size, stride, device, use_amp=True):\n",
    "    \"\"\"\n",
    "    Perform sliding-window (patch-based) inference on a 5D tensor.\n",
    "    \n",
    "    Args:\n",
    "      volume (torch.Tensor): Input volume of shape (1, C, D, H, W).\n",
    "      model (torch.nn.Module): The segmentation model.\n",
    "      patch_size (tuple): The patch size as (pD, pH, pW).\n",
    "      stride (tuple): The stride (step) for sliding the window (sD, sH, sW).\n",
    "      device (torch.device): The device for inference.\n",
    "      use_amp (bool): Whether to use AMP (mixed precision).\n",
    "    \n",
    "    Returns:\n",
    "      aggregated_logits (torch.Tensor): Aggregated output logits of shape \n",
    "          (1, out_channels, D, H, W), averaged over overlapping patches.\n",
    "    \"\"\"\n",
    "    _, C, D, H, W = volume.shape\n",
    "    pD, pH, pW = patch_size\n",
    "    sD, sH, sW = stride\n",
    "\n",
    "    # Run a dummy patch through the model to get number of output channels.\n",
    "    with torch.no_grad():\n",
    "        dummy_patch = volume[:, :, :pD, :pH, :pW].to(device)\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                dummy_out = model(dummy_patch)\n",
    "        else:\n",
    "            dummy_out = model(dummy_patch)\n",
    "        out_channels = dummy_out.shape[1]\n",
    "\n",
    "    # Create tensors to accumulate logits and a counter for overlap.\n",
    "    aggregated_logits = torch.zeros((1, out_channels, D, H, W), device=device)\n",
    "    count_map = torch.zeros((1, 1, D, H, W), device=device)\n",
    "\n",
    "    # Loop over the volume with the given stride.\n",
    "    for d in range(0, D, sD):\n",
    "        for h in range(0, H, sH):\n",
    "            for w in range(0, W, sW):\n",
    "                d_start = d\n",
    "                h_start = h\n",
    "                w_start = w\n",
    "                d_end = min(d_start + pD, D)\n",
    "                h_end = min(h_start + pH, H)\n",
    "                w_end = min(w_start + pW, W)\n",
    "                \n",
    "                # Extract the patch.\n",
    "                patch = volume[:, :, d_start:d_end, h_start:h_end, w_start:w_end]\n",
    "                # Determine needed padding (pad only on the end sides).\n",
    "                pad_d = pD - patch.shape[2]\n",
    "                pad_h = pH - patch.shape[3]\n",
    "                pad_w = pW - patch.shape[4]\n",
    "                if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "                    # F.pad expects pad in the order: (w_left, w_right, h_left, h_right, d_left, d_right)\n",
    "                    patch = F.pad(patch, (0, pad_w, 0, pad_h, 0, pad_d))2024 21st International Conference on Ubiquitous Robots (UR), 176-183\t\n",
    "                with torch.no_grad():\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                            patch_logits = model(patch)\n",
    "                    else:\n",
    "                        patch_logits = model(patch)\n",
    "                \n",
    "                # Remove any extra padded predictions.\n",
    "                actual_d = d_end - d_start\n",
    "                actual_h = h_end - h_start\n",
    "                actual_w = w_end - w_start\n",
    "                patch_logits = patch_logits[:, :, :actual_d, :actual_h, :actual_w]\n",
    "                \n",
    "                # Accumulate logits and update count map.\n",
    "                aggregated_logits[:, :, d_start:d_end, h_start:h_end, w_start:w_end] += patch_logits\n",
    "                count_map[:, :, d_start:d_end, h_start:h_end, w_start:w_end] += 1\n",
    "\n",
    "    # Average overlapping regions.\n",
    "    aggregated_logits = aggregated_logits / count_map\n",
    "    return aggregated_logits\n",
    "\n",
    "def infer_full_volume(volume_np, model, patch_size=(64,64,64), stride=(32,32,32),\n",
    "                      device=torch.device(\"cuda\"), use_amp=True):\n",
    "    \"\"\"\n",
    "    Given a full volume (numpy array of shape (D, H, W)), perform patch-based inference\n",
    "    using the model and return the predicted segmentation mask.\n",
    "    \"\"\"\n",
    "    # Convert to tensor with shape (1, 1, D, H, W)\n",
    "    volume_tensor = torch.from_numpy(volume_np).unsqueeze(0).unsqueeze(0)\n",
    "    volume_tensor = volume_tensor.to(device, dtype=torch.float32)\n",
    "    \n",
    "    aggregated_logits = sliding_window_inference(volume_tensor, model, patch_size, stride, device, use_amp)\n",
    "    # Argmax to get final segmentation mask (shape: (1, D, H, W))\n",
    "    pred_mask = torch.argmax(aggregated_logits, dim=1)\n",
    "    return pred_mask.cpu().numpy().squeeze(0)  # shape: (D, H, W)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Inference and Visualization Function\n",
    "# =============================================================================\n",
    "def visualize_inference():\n",
    "    \"\"\"\n",
    "    Loads full test volumes from a test directory, runs patch-based inference using\n",
    "    mixed precision, applies the predicted mask to create a masked image, and saves\n",
    "    a PDF file with side-by-side visualization (center axial slice) of the original \n",
    "    and masked images.\n",
    "    \"\"\"\n",
    "    # ----- Setup paths and device -----\n",
    "    test_folder = os.path.join(ROOT_DIR, \"imagesTs\")\n",
    "    test_images = sorted(glob(os.path.join(test_folder, \"*.nii.gz\")))\n",
    "    output_pdf = \"inference_results.pdf\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # ----- Load the trained model -----\n",
    "    # (Assuming UNet3D is already defined and imported)\n",
    "    model = UNet3D(in_channels=1, out_channels=2, base_filters=32, bilinear=True)\n",
    "    CHECKPOINT_PATH = \"checkpoints/checkpoint_epoch_100.pth\"  # Adjust path as needed\n",
    "    # You may consider using weights_only=True if available and safe.\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # ----- Set patch-based inference parameters -----\n",
    "    patch_size = (64, 64, 64)  # Smaller patches to fit in memory\n",
    "    stride = (32, 32, 32)       # Overlap between patches\n",
    "    \n",
    "    # ----- Prepare PDF for visualization -----\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for idx, img_path in enumerate(test_images):\n",
    "            base_name = os.path.basename(img_path)\n",
    "            print(f\"Processing {base_name} with patch-based inference...\")\n",
    "            \n",
    "            # Load the full volume (shape: D x H x W)\n",
    "            img_nifti = nib.load(img_path)\n",
    "            img_np = img_nifti.get_fdata(dtype=np.float32)\n",
    "            \n",
    "            # Run patch-based inference to obtain predicted segmentation mask.\n",
    "            pred_mask_np = infer_full_volume(img_np, model, patch_size, stride, device, use_amp=True)\n",
    "            \n",
    "            # Create a masked image (zeroing out background)\n",
    "            masked_np = img_np * pred_mask_np\n",
    "            \n",
    "            # For visualization, we take the center axial slice.\n",
    "            D, H, W = img_np.shape\n",
    "            z_mid = D // 2\n",
    "            original_slice = img_np[z_mid, :, :]\n",
    "            masked_slice = masked_np[z_mid, :, :]\n",
    "            \n",
    "            # Create a figure with 2 subplots: original and masked.\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            fig.suptitle(f\"{base_name} - Center Axial Slice (z={z_mid})\", fontsize=16)\n",
    "            axs[0].imshow(original_slice, cmap='gray')\n",
    "            axs[0].set_title(\"Original\")\n",
    "            axs[1].imshow(masked_slice, cmap='gray')\n",
    "            axs[1].set_title(\"Masked\")\n",
    "            for ax in axs:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            print(f\"Finished processing {base_name}\")\n",
    "            \n",
    "    print(f\"Inference results saved to: {output_pdf}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Main Guard\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm  # Progress tracking\n",
    "import torchvision.transforms.functional as TF\n",
    "TARGET_SIZE = (256, 256)  # Standardize all slices to this size\n",
    "\n",
    "class SlicesDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths=None, axis=0, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.axis = axis\n",
    "        self.transform = transform\n",
    "        self.slice_index_mapping = []  # Stores (file_idx, slice_idx)\n",
    "\n",
    "        print(\"Indexing slices from NIfTI files...\")\n",
    "        \n",
    "        # Create index mapping without loading files into memory\n",
    "        for file_idx, img_path in enumerate(tqdm(self.image_paths, desc=\"Indexing\", unit=\"file\")):\n",
    "            img_nifti = nib.load(img_path)\n",
    "            img_shape = img_nifti.shape  # Get shape without loading into memory\n",
    "            num_slices = img_shape[self.axis]\n",
    "\n",
    "            for slice_idx in range(num_slices):\n",
    "                self.slice_index_mapping.append((file_idx, slice_idx))  # Store file & slice index\n",
    "\n",
    "        print(f\"Total slices indexed: {len(self.slice_index_mapping)}\")\n",
    "\n",
    "        # Optional: Cache the last loaded file to avoid redundant loading\n",
    "        self.last_loaded_file = None\n",
    "        self.last_loaded_data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_index_mapping)\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, slice_idx = self.slice_index_mapping[idx]\n",
    "        img_path = self.image_paths[file_idx]\n",
    "        label_path = self.label_paths[file_idx] if self.label_paths else None\n",
    "\n",
    "        # Load image\n",
    "        if self.last_loaded_file != img_path:\n",
    "            img_nifti = nib.load(img_path)\n",
    "            self.last_loaded_data = img_nifti.get_fdata(dtype=np.float32)\n",
    "            self.last_loaded_file = img_path\n",
    "\n",
    "        img_3d = self.last_loaded_data\n",
    "        img_slice = img_3d.take(slice_idx, axis=self.axis)\n",
    "\n",
    "        # Normalize safely\n",
    "        if img_slice.max() - img_slice.min() > 0:\n",
    "            img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())\n",
    "        else:\n",
    "            img_slice = np.zeros_like(img_slice)  # Avoid NaNs\n",
    "\n",
    "        # Convert to tensor and resize\n",
    "        img_tensor = torch.from_numpy(img_slice[np.newaxis, ...]).float()  # Shape: (1, H, W)\n",
    "        img_tensor = TF.resize(img_tensor, TARGET_SIZE)  # Resize to fixed shape\n",
    "\n",
    "        # Process label (if available)\n",
    "        label_tensor = None\n",
    "        if label_path:\n",
    "            if self.last_loaded_file != label_path:\n",
    "                label_nifti = nib.load(label_path)\n",
    "                label_3d = label_nifti.get_fdata(dtype=np.float32).astype(np.int32)\n",
    "                self.last_loaded_data = label_3d\n",
    "                self.last_loaded_file = label_path\n",
    "            label_slice = self.last_loaded_data.take(slice_idx, axis=self.axis)\n",
    "\n",
    "            # Convert label to tensor and resize\n",
    "            label_tensor = torch.from_numpy(label_slice).long()\n",
    "            label_tensor = TF.resize(label_tensor.unsqueeze(0), TARGET_SIZE).squeeze(0)\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv2D(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper module that performs:\n",
    "    (conv2d -> ReLU -> conv2d -> ReLU)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv2D, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Downscaling with maxpool then DoubleConv2D.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down2D, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv2D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Up2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling then DoubleConv2D.\n",
    "    If `bilinear` is True, use nn.Upsample for upscaling.\n",
    "    Otherwise, use ConvTranspose2d.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up2D, self).__init__()\n",
    "        self.bilinear = bilinear\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv2D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1: decoder feature map\n",
    "        # x2: skip connection from encoder\n",
    "        x1 = self.up(x1)\n",
    "        # Match x1 size to x2 (in case of odd dimensions)\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2D U-Net implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=32, bilinear=True):\n",
    "        super(UNet2D, self).__init__()\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv2D(in_channels, base_filters)\n",
    "        self.down1 = Down2D(base_filters, base_filters * 2)\n",
    "        self.down2 = Down2D(base_filters * 2, base_filters * 4)\n",
    "        self.down3 = Down2D(base_filters * 4, base_filters * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down2D(base_filters * 8, base_filters * 16 // factor)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = Up2D(base_filters * 16, base_filters * 8 // factor, bilinear)\n",
    "        self.up2 = Up2D(base_filters * 8, base_filters * 4 // factor, bilinear)\n",
    "        self.up3 = Up2D(base_filters * 4, base_filters * 2 // factor, bilinear)\n",
    "        self.up4 = Up2D(base_filters * 2, base_filters, bilinear)\n",
    "\n",
    "        # Final layer\n",
    "        self.outc = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Indexing slices from NIfTI files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|██████████| 20/20 [00:00<00:00, 2038.89file/s]\n",
      "/tmp/ipykernel_74444/3454813748.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total slices indexed: 6400\n",
      "\n",
      "Starting Epoch 1/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/800 [00:00<?, ?batch/s]/tmp/ipykernel_74444/3454813748.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/10: 100%|██████████| 800/800 [15:08<00:00,  1.14s/batch, dice=0.0000, loss=0.0046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.0471, Average Dice: 0.2939\n",
      "\n",
      "Starting Epoch 2/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 800/800 [15:22<00:00,  1.15s/batch, dice=0.0000, loss=0.0101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average Loss: 0.0114, Average Dice: 0.3150\n",
      "\n",
      "Starting Epoch 3/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 800/800 [15:30<00:00,  1.16s/batch, dice=0.4279, loss=0.0024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Average Loss: 0.0058, Average Dice: 0.6203\n",
      "\n",
      "Starting Epoch 4/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 800/800 [15:11<00:00,  1.14s/batch, dice=1.0000, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Average Loss: 0.0036, Average Dice: 0.7354\n",
      "\n",
      "Starting Epoch 5/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 800/800 [15:25<00:00,  1.16s/batch, dice=1.0000, loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Average Loss: 0.0027, Average Dice: 0.8091\n",
      "\n",
      "Starting Epoch 6/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 800/800 [15:20<00:00,  1.15s/batch, dice=1.0000, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Average Loss: 0.0021, Average Dice: 0.8358\n",
      "\n",
      "Starting Epoch 7/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 800/800 [15:23<00:00,  1.15s/batch, dice=0.8798, loss=0.0030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Average Loss: 0.0018, Average Dice: 0.8596\n",
      "\n",
      "Starting Epoch 8/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 800/800 [15:22<00:00,  1.15s/batch, dice=0.9453, loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Average Loss: 0.0016, Average Dice: 0.8828\n",
      "\n",
      "Starting Epoch 9/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 800/800 [15:24<00:00,  1.16s/batch, dice=0.8670, loss=0.0036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Average Loss: 0.0015, Average Dice: 0.8809\n",
      "\n",
      "Starting Epoch 10/10\n",
      "Number of batches in train_loader: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 800/800 [15:23<00:00,  1.15s/batch, dice=0.9294, loss=0.0014]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Average Loss: 0.0014, Average Dice: 0.8886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to compute Dice Similarity Coefficient (DSC)\n",
    "def dice_score(pred, target, smooth=1e-5):\n",
    "    pred = (pred > 0.5).float()  # Convert probabilities to binary mask\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate dataset & dataloader\n",
    "train_dataset = SlicesDataset(\n",
    "    image_paths=train_image_paths,\n",
    "    label_paths=train_label_paths,\n",
    "    axis=0  # Axial slicing\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Initialize UNet model\n",
    "model = UNet2D(in_channels=1, out_channels=2, base_filters=32, bilinear=True).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Mixed Precision (AMP) for faster training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Adjust as needed\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_dice = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    print(f\"\\nStarting Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "        for slices, labels in train_loader:\n",
    "            # Move data to GPU (if available)\n",
    "            slices = slices.to(device, dtype=torch.float32)  # Shape: (B, 1, H, W)\n",
    "            labels = labels.to(device, dtype=torch.long)  # Shape: (B, H, W)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Enable mixed precision (AMP)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(slices)  # Shape: (B, C, H, W)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation with AMP\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Compute Dice score\n",
    "            with torch.no_grad():\n",
    "                preds = torch.argmax(outputs, dim=1)  # Convert logits to predicted labels\n",
    "                dice = dice_score(preds, labels)\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_dice += dice.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", dice=f\"{dice.item():.4f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Compute average loss and Dice score\n",
    "    avg_loss = epoch_loss / batch_count\n",
    "    avg_dice = epoch_dice / batch_count\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}, Average Dice: {avg_dice:.4f}\")\n",
    "\n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = f\"checkpoints/model_epoch_{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final trained model saved: /home/raghuram/ARPL/MR-Image-Reconstruction-Using-Deep-Learning/Data/checkpoints/final_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save final model after training\n",
    "final_model_path = os.path.join(\"/home/raghuram/ARPL/MR-Image-Reconstruction-Using-Deep-Learning/Data/checkpoints\", \"final_model.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final trained model saved: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74444/1587637971.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing la_001.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_001.nii.gz\n",
      "Processing la_002.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_002.nii.gz\n",
      "Processing la_006.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_006.nii.gz\n",
      "Processing la_008.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_008.nii.gz\n",
      "Processing la_012.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_012.nii.gz\n",
      "Processing la_013.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_013.nii.gz\n",
      "Processing la_015.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_015.nii.gz\n",
      "Processing la_025.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_025.nii.gz\n",
      "Processing la_027.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_027.nii.gz\n",
      "Processing la_028.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_028.nii.gz\n",
      "Inference results saved to: inference_results_2d.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 2D Slice-wise Inference Function\n",
    "# =============================================================================\n",
    "def infer_2d_slices(volume_np, model, device):\n",
    "    \"\"\"\n",
    "    Performs inference on each axial slice of a 3D volume using a trained 2D UNet model.\n",
    "\n",
    "    Args:\n",
    "      volume_np (numpy.ndarray): Input 3D volume (D, H, W).\n",
    "      model (torch.nn.Module): Trained 2D UNet model.\n",
    "      device (torch.device): The device for inference.\n",
    "\n",
    "    Returns:\n",
    "      pred_mask_np (numpy.ndarray): Predicted segmentation mask (D, H, W).\n",
    "    \"\"\"\n",
    "    D, H, W = volume_np.shape\n",
    "    pred_mask_np = np.zeros((D, H, W), dtype=np.uint8)\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    volume_tensor = torch.from_numpy(volume_np).float()\n",
    "    volume_tensor = (volume_tensor - volume_tensor.min()) / (volume_tensor.max() - volume_tensor.min())  # Normalize\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for z in range(D):\n",
    "            slice_2d = volume_tensor[z, :, :].unsqueeze(0).unsqueeze(0).to(device)  # Shape: (1, 1, H, W)\n",
    "            \n",
    "            # Resize slice if necessary\n",
    "            target_size = (256, 256)\n",
    "            slice_2d_resized = TF.resize(slice_2d, target_size)\n",
    "            \n",
    "            # Inference\n",
    "            logits = model(slice_2d_resized)  # Shape: (1, num_classes, H, W)\n",
    "            pred_mask = torch.argmax(logits, dim=1).cpu().numpy().squeeze(0)  # Shape: (H, W)\n",
    "\n",
    "            # Resize back to original size\n",
    "            pred_mask_np[z, :, :] = TF.resize(torch.from_numpy(pred_mask).unsqueeze(0), (H, W)).squeeze(0).numpy()\n",
    "    \n",
    "    return pred_mask_np\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Inference and Visualization Function\n",
    "# =============================================================================\n",
    "def visualize_2d_inference():\n",
    "    \"\"\"\n",
    "    Loads test volumes from a test directory, runs slice-wise inference using a \n",
    "    2D UNet model, and saves a PDF showing original and predicted segmentation.\n",
    "    \"\"\"\n",
    "    # ----- Setup paths and device -----\n",
    "    test_folder = os.path.join(ROOT_DIR, \"imagesTs\")\n",
    "    test_images = sorted(glob(os.path.join(test_folder, \"*.nii.gz\")))\n",
    "    output_pdf = \"inference_results_2d.pdf\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ----- Load the trained 2D model -----\n",
    "    model = UNet2D(in_channels=1, out_channels=2, base_filters=32, bilinear=True)\n",
    "    CHECKPOINT_PATH = \"checkpoints/final_model.pth\"  # Adjust path if needed\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # ----- Prepare PDF for visualization -----\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for idx, img_path in enumerate(test_images):\n",
    "            base_name = os.path.basename(img_path)\n",
    "            print(f\"Processing {base_name} with slice-wise 2D inference...\")\n",
    "\n",
    "            # Load full volume (D, H, W)\n",
    "            img_nifti = nib.load(img_path)\n",
    "            img_np = img_nifti.get_fdata(dtype=np.float32)\n",
    "            \n",
    "            # Run slice-wise 2D inference\n",
    "            pred_mask_np = infer_2d_slices(img_np, model, device)\n",
    "            \n",
    "            # Create masked image\n",
    "            masked_np = img_np * pred_mask_np\n",
    "\n",
    "            # Visualize center axial slice\n",
    "            D, H, W = img_np.shape\n",
    "            z_mid = D // 2\n",
    "            original_slice = img_np[z_mid, :, :]\n",
    "            masked_slice = masked_np[z_mid, :, :]\n",
    "\n",
    "            # Create a figure with 2 subplots: original and masked.\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            fig.suptitle(f\"{base_name} - Center Axial Slice (z={z_mid})\", fontsize=16)\n",
    "            axs[0].imshow(original_slice, cmap='gray')\n",
    "            axs[0].set_title(\"Original\")\n",
    "            axs[1].imshow(masked_slice, cmap='gray')\n",
    "            axs[1].set_title(\"Masked\")\n",
    "            for ax in axs:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            print(f\"Finished processing {base_name}\")\n",
    "            \n",
    "    print(f\"Inference results saved to: {output_pdf}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Main Guard\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_2d_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74444/1705110956.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing la_001.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_001.nii.gz\n",
      "Processing la_002.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_002.nii.gz\n",
      "Processing la_006.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_006.nii.gz\n",
      "Processing la_008.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_008.nii.gz\n",
      "Processing la_012.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_012.nii.gz\n",
      "Processing la_013.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_013.nii.gz\n",
      "Processing la_015.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_015.nii.gz\n",
      "Processing la_025.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_025.nii.gz\n",
      "Processing la_027.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_027.nii.gz\n",
      "Processing la_028.nii.gz with slice-wise 2D inference...\n",
      "Finished processing la_028.nii.gz\n",
      "Inference results saved to: inference_results_2d.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 2D Slice-wise Inference Function\n",
    "# =============================================================================\n",
    "def infer_2d_slices(volume_np, model, device):\n",
    "    \"\"\"\n",
    "    Performs inference on each axial slice of a 3D volume using a trained 2D UNet model.\n",
    "\n",
    "    Args:\n",
    "      volume_np (numpy.ndarray): Input 3D volume (D, H, W).\n",
    "      model (torch.nn.Module): Trained 2D UNet model.\n",
    "      device (torch.device): The device for inference.\n",
    "\n",
    "    Returns:\n",
    "      pred_mask_np (numpy.ndarray): Predicted segmentation mask (D, H, W).\n",
    "    \"\"\"\n",
    "    D, H, W = volume_np.shape\n",
    "    pred_mask_np = np.zeros((D, H, W), dtype=np.uint8)\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    volume_tensor = torch.from_numpy(volume_np).float()\n",
    "    volume_tensor = (volume_tensor - volume_tensor.min()) / (volume_tensor.max() - volume_tensor.min())  # Normalize\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for z in range(D):\n",
    "            slice_2d = volume_tensor[z, :, :].unsqueeze(0).unsqueeze(0).to(device)  # Shape: (1, 1, H, W)\n",
    "            \n",
    "            # Resize slice if necessary\n",
    "            target_size = (256, 256)\n",
    "            slice_2d_resized = TF.resize(slice_2d, target_size)\n",
    "            \n",
    "            # Inference\n",
    "            logits = model(slice_2d_resized)  # Shape: (1, num_classes, H, W)\n",
    "            pred_mask = torch.argmax(logits, dim=1).cpu().numpy().squeeze(0)  # Shape: (H, W)\n",
    "\n",
    "            # Resize back to original size\n",
    "            pred_mask_np[z, :, :] = TF.resize(torch.from_numpy(pred_mask).unsqueeze(0), (H, W)).squeeze(0).numpy()\n",
    "    \n",
    "    return pred_mask_np\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Inference and Visualization Function\n",
    "# =============================================================================\n",
    "def visualize_2d_inference():\n",
    "    \"\"\"\n",
    "    Loads test volumes from a test directory, runs slice-wise inference using a \n",
    "    2D UNet model, and saves a PDF showing original images with red overlay segmentation.\n",
    "    \"\"\"\n",
    "    # ----- Setup paths and device -----\n",
    "    test_folder = os.path.join(ROOT_DIR, \"imagesTs\")\n",
    "    test_images = sorted(glob(os.path.join(test_folder, \"*.nii.gz\")))\n",
    "    output_pdf = \"inference_results_2d.pdf\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ----- Load the trained 2D model -----\n",
    "    model = UNet2D(in_channels=1, out_channels=2, base_filters=32, bilinear=True)\n",
    "    CHECKPOINT_PATH = \"checkpoints/final_model.pth\"  # Adjust path if needed\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # ----- Prepare PDF for visualization -----\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for idx, img_path in enumerate(test_images):\n",
    "            base_name = os.path.basename(img_path)\n",
    "            print(f\"Processing {base_name} with slice-wise 2D inference...\")\n",
    "\n",
    "            # Load full volume (D, H, W)\n",
    "            img_nifti = nib.load(img_path)\n",
    "            img_np = img_nifti.get_fdata(dtype=np.float32)\n",
    "            \n",
    "            # Run slice-wise 2D inference\n",
    "            pred_mask_np = infer_2d_slices(img_np, model, device)\n",
    "            \n",
    "            # Visualize center axial slice\n",
    "            D, H, W = img_np.shape\n",
    "            z_mid = D // 2\n",
    "            original_slice = img_np[z_mid, :, :]\n",
    "            mask_slice = pred_mask_np[z_mid, :, :]\n",
    "\n",
    "            # ----- Create an overlay where the mask is red -----\n",
    "            img_rgb = np.stack([original_slice] * 3, axis=-1)  # Convert grayscale to RGB\n",
    "            img_rgb = (img_rgb - img_rgb.min()) / (img_rgb.max() - img_rgb.min())  # Normalize to [0, 1]\n",
    "            \n",
    "            mask_rgb = np.zeros_like(img_rgb)\n",
    "            mask_rgb[:, :, 0] = mask_slice  # Set red channel to mask\n",
    "            mask_overlay = (img_rgb * 0.5) + (mask_rgb * 0.5)  # Blend 50% red mask\n",
    "\n",
    "            # Create a figure with 2 subplots: original and masked.\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            fig.suptitle(f\"{base_name} - Center Axial Slice (z={z_mid})\", fontsize=16)\n",
    "            axs[0].imshow(original_slice, cmap='gray')\n",
    "            axs[0].set_title(\"Original\")\n",
    "            axs[1].imshow(mask_overlay)\n",
    "            axs[1].set_title(\"Masked (Red Overlay)\")\n",
    "            for ax in axs:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            print(f\"Finished processing {base_name}\")\n",
    "            \n",
    "    print(f\"Inference results saved to: {output_pdf}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Main Guard\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_2d_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
