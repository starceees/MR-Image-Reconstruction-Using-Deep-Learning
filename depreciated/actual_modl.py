# -*- coding: utf-8 -*-
"""Actual_MODL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TJgcLBGFhuFr-Wt3IRzXzWmZFgdKjE2f
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.nn.parameter as Parameter

from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
import numpy as np
from datetime import datetime
from tqdm import tqdm
import time
import numpy as np
import h5py as h5
from torch.utils.data import DataLoader, TensorDataset


# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def generateUndersampled(org,csm,mask,sigma=0.05):
    nSlice,ncoil,nrow,ncol=csm.shape
    atb=np.empty(org.shape,dtype=np.complex64)
    for i in range(nSlice):
        A  = lambda z: piA(z,csm[i],mask[i],nrow,ncol,ncoil)
        At = lambda z: piAt(z,csm[i],mask[i],nrow,ncol,ncoil)

        sidx=np.where(mask[i].ravel()!=0)[0]
        nSIDX=len(sidx)
        noise=np.random.randn(nSIDX*ncoil,)+1j*np.random.randn(nSIDX*ncoil,)
        noise=noise*(sigma/np.sqrt(2.))
        y=A(org[i]) + noise
        atb[i]=At(y)
    return atb

def div0(a, b):
    """ This function handles division by zero """
    c = np.divide(a, b, out=np.zeros_like(a), where=b != 0)
    return c

def TicTocGenerator():
    # Generator that returns time differences
    ti = 0           # initial time
    tf = time.time() # final time
    while True:
        ti = tf
        tf = time.time()
        yield tf - ti  # returns the time difference

TicToc = TicTocGenerator()  # create an instance of the TicTocGen generator

def toc(tempBool=True):
    # Prints the time difference yielded by generator instance TicToc
    tempTimeInterval = next(TicToc)
    if tempBool:
        print("Elapsed time: %f seconds.\n" % tempTimeInterval)

def tic():
    # Records a time in TicToc, marks the beginning of a time interval
    toc(False)

def normalize01(img):
    """
    Normalize the image between 0 and 1
    """
    if len(img.shape) == 3:
        nimg = len(img)
    else:
        nimg = 1
        r, c = img.shape
        img = np.reshape(img, (nimg, r, c))
    img2 = np.empty(img.shape, dtype=img.dtype)
    for i in range(nimg):
        img2[i] = div0(img[i] - img[i].min(), img[i].ptp())
        # img2[i] = (img[i] - img[i].min()) / (img[i].max() - img[i].min())
    return np.squeeze(img2).astype(img.dtype)

def np_crop(data, shape=(320, 320)):
    w_from = (data.shape[-2] - shape[0]) // 2
    h_from = (data.shape[-1] - shape[1]) // 2
    w_to = w_from + shape[0]
    h_to = h_from + shape[1]
    return data[..., w_from:w_to, h_from:h_to]

def myPSNR(org, recon):
    """ This function calculates PSNR between the original and
    the reconstructed images"""
    mse = np.sum(np.square(np.abs(org - recon))) / org.size
    psnr = 20 * np.log10(org.max() / (np.sqrt(mse) + 1e-10))
    return psnr

def getData(trnTst='testing', num=100, sigma=0.01):
    print('Reading the data. Please wait...')
    filename = "/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5"  # set the correct path here
    # filename='/Users/haggarwal/datasets/piData/dataset.hdf5'

    tic()
    with h5.File(filename) as f:
        if trnTst == 'training':
            org, csm, mask = f['trnOrg'][:], f['trnCsm'][:], f['trnMask'][:]
        else:
            org, csm, mask = f['tstOrg'][num], f['tstCsm'][num], f['tstMask'][num]
            na = np.newaxis
            org, csm, mask = org[na], csm[na], mask[na]
    toc()

    print('Successfully read the data from file!')
    print('Now doing undersampling....')
    tic()
    atb = generateUndersampled(org, csm, mask, sigma)
    toc()
    print('Successfully undersampled data!')
    if trnTst == 'testing':
        atb = c2r(atb)
    return org, atb, csm, mask

def r2c(inp):
    """  input img: row x col x 2 in float32
    output image: row  x col in complex64
    """
    if inp.dtype == 'float32':
        dtype = torch.complex64
    else:
        dtype = torch.complex128
    out = torch.zeros(inp.shape[0:2], dtype=dtype)
    out = inp[..., 0] + 1j * inp[..., 1]
    return out

def c2r(inp):
    """ Convert complex PyTorch tensor to a real-imaginary format tensor """
    if inp.is_complex():
        # Convert complex tensor to real-imaginary format
        return torch.view_as_real(inp)
    else:
        # If input is not complex, return as is or handle appropriately
        return inp
    if inp.dtype == 'complex64':
        dtype = torch.float32
    else:
        dtype = torch.float64
    out = torch.zeros(inp.shape + (2,), dtype=dtype)

    # # Convert numpy arrays to PyTorch tensors before assignment
    real_tensor = torch.from_numpy(inp.real).to(dtype)
    imag_tensor = torch.from_numpy(inp.imag).to(dtype)

    out[..., 0] = real_tensor
    out[..., 1] = imag_tensor
    return out
def piA(x,csm,mask,nrow,ncol,ncoil):
    """ This is a the A operator as defined in the paper"""
    ccImg=np.reshape(x,(nrow,ncol) )
    coilImages=np.tile(ccImg,[ncoil,1,1])*csm;
    kspace=np.fft.fft2(coilImages)/np.sqrt(nrow * ncol)
    if len(mask.shape)==2:
        mask=np.tile(mask,(ncoil,1,1))
    res=kspace[mask!=0]
    return res

def piAt(kspaceUnder,csm,mask,nrow,ncol,ncoil):
    """ This is a the A^T operator as defined in the paper"""
    temp=np.zeros((ncoil,nrow,ncol),dtype=np.complex64)
    if len(mask.shape)==2:
        mask=np.tile(mask,(ncoil,1,1))

    temp[mask!=0]=kspaceUnder
    img=np.fft.ifft2(temp)*np.sqrt(nrow*ncol)
    coilComb=np.sum(img*np.conj(csm),axis=0).astype(np.complex64)
    #coilComb=coilComb.ravel();
    return coilComb

# Define the model

class ComplexConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, padding):
        super(ComplexConv2d, self).__init__()
        # Assuming the input will be split into real and imaginary parts
        self.real_conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)
        self.imag_conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)

    def forward(self, x):
        # Splitting the input into real and imaginary components
        real_part, imag_part = torch.chunk(x, 2, dim=1)
        print(f"Real Part : {real_part.shape}")
        print(f"Imag Part : {imag_part.shape}")
        real = self.real_conv(real_part) - self.imag_conv(imag_part)
        imag = self.real_conv(imag_part) + self.imag_conv(real_part)
        return torch.cat([real, imag], dim=1)

class ComplexReLU(nn.Module):
    def forward(self, x):
        real_part = torch.relu(x[:, :, :, 0])  # Apply ReLU to the real part
        imag_part = torch.relu(x[:, :, :, 1])  # Apply ReLU to the imaginary part
        return torch.stack([real_part, imag_part], dim=-1)

class DwBlock(nn.Module):
    def __init__(self, n_layers, output_channels):
        super(DwBlock, self).__init__()
        self.layers = nn.ModuleList([ComplexConv2d(128, 128, kernel_size=3, padding=1)] +
                                    [ComplexConv2d(256, 256, kernel_size=3, padding=1) for _ in range(1, n_layers - 1)] +
                                    [ComplexConv2d(256, output_channels, kernel_size=3, padding=1)])
        self.n_layers = n_layers
        self.layers = nn.ModuleList()

        for i in range(1, n_layers + 1):
            self.layers.append(self._create_layer(i))

    def _create_layer(self, i):
        return ComplexConv2d(128, 128, kernel_size=3, padding=1) if i != self.n_layers else ComplexConv2d(128, 2, kernel_size=3, padding=1)

    def forward(self, x):
        for layer in self.layers:
            x = ComplexReLU()(layer(x))
        return x

class Aclass(nn.Module):
    def __init__(self, csm, mask, lam):
        super(Aclass, self).__init__()
        self.nrow, self.ncol = mask.shape[-2:]
        self.pixels = self.nrow * self.ncol
        self.mask = mask
        self.csm = csm
        self.SF = torch.complex(torch.sqrt(torch.tensor([self.pixels], dtype=torch.float32)), torch.tensor([0.], dtype=torch.float32))
        self.lam = lam

    def myAtA(self, img):
        print(f"Size of img: {img.size()}")
        print(f"Size of self.csm: {self.csm.size()}")
        original_shape = img.shape
        img = img.view(img.size(0), 1, 1, 1)
        coil_images = img * self.csm
        self.SF = self.SF.to(coil_images.device)
        kspace = torch.fft.ifft2(coil_images) / self.SF
        temp = kspace * self.mask[:, None, :, :]  # Add an extra dimension for broadcasting
        coil_imgs = torch.fft.fft2(temp) * self.SF
        coil_comb = torch.sum(coil_imgs * torch.conj(self.csm), axis=0)
        coil_comb = coil_comb + self.lam * img
        assert img.shape == original_shape, f"Shape altered in myAtA. Original: {original_shape}, Now: {img.shape}"
        return coil_comb

class MoDLModel(nn.Module):
    def __init__(self, n_layers, k, gradient_method, output_channels):
        super(MoDLModel, self).__init__()
        self.n_layers = n_layers
        self.k = k
        self.gradient_method = gradient_method
        self.dw_block = DwBlock(n_layers, output_channels)

    # ...
    def forward(self, atb, csm, mask):
        dc_input = atb
        for i in range(1, self.k + 1):
            dw_output = self.dw_block(dc_input)
            lam = nn.Parameter(torch.full((64,1 , 1, 1), 0.05, dtype=torch.float32)).to(dw_output.device)
            print(f"Size of Lam variable {i}: {lam.size()}")
            print(f"Size of atb at iteration {i}: {atb.size()}")
            print(f"Size of dw_output at iteration {i}: {dw_output.size()}")
            test = lam * dw_output
            print(f"Size of test at iteration {i}: {test.size()}")
            self.expansion_layer = nn.Conv2d(4, 256, kernel_size=1)  # 1x1 convolution to change channels
            self.expansion_layer = self.expansion_layer.to(device)
            # In your forward method

            expanded_output = self.expansion_layer(dw_output)  # Adjust channel size
            test = expanded_output * lam

            rhs =atb + test
            assert dc_input.shape == atb.shape, f"Shape mismatch after dw_block at iteration {i}"
            if self.gradient_method == 'AG':
                dc_output = self.dc(rhs, csm, mask, lam)
            elif self.gradient_method == 'MG':
                if self.training:
                    dc_output = self.dc_manual_gradient(rhs)
                else:
                    dc_output = self.dc(rhs, csm, mask, lam)
            dc_input = dc_output
        return dc_input

    def dc(self, rhs, csm, mask, lam1):
        lam2 = lam1

        a_obj = Aclass(csm, mask, lam2)
        y = self.cg(a_obj, rhs)
        return y

    def dc_manual_gradient(self, x):
        return self.cg(x)

    def cg(self, a_obj, rhs):
        rhs = rhs.permute(0, 3, 1, 2)
        #print(f"rhs:{rhs}")
        #rhs = self.c2r(rhs)
        #rhs = rhs.view(rhs.size(0), -1)
        rhs = rhs.permute(1, 0)
        y = self.call_cg(rhs, a_obj)
        y = y.permute(1, 0)
        #y = y.view(y.size(0), 2, self.nrow, self.ncol)
        y = y.permute(0, 2, 3, 1)
        return y

    def call_cg(self, rhs, a_obj):
        y = torch.empty_like(rhs)
        for i in range(rhs.size(0)):
            a = a_obj.myAtA(rhs[i])
            y[i] = self.my_cg(a_obj, a, rhs[i])
        return y

    def my_cg(self, a_obj, a, rhs):
        x = torch.zeros_like(rhs)
        r = rhs.clone()
        p = r.clone()
        rTr = torch.sum(torch.conj(r) * r)

        i = 0
        while i < 10 and torch.any(torch.abs(rTr) > 1e-10):

            Ap = a_obj.myAtA(p)

            p_expanded = p.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(Ap)

            # Then perform the operation
            alpha = rTr / torch.sum(torch.conj(p_expanded) * Ap, dim=(1, 2, 3))

            # Reshape r to match the dimensions of Ap for broadcasting
            r_expanded = r.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(Ap)

            x = x + alpha.unsqueeze(1).unsqueeze(2).unsqueeze(3) * p
            r = r_expanded - alpha.unsqueeze(1).unsqueeze(2).unsqueeze(3) * Ap

            beta = torch.sum(torch.conj(r) * r, dim=(1, 2, 3)) / rTr
            # Reshape p to match the dimensions of r for broadcasting
            p_expanded = p.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(r)

            # Then perform the operation
            p = r + beta.unsqueeze(1).unsqueeze(2).unsqueeze(3) * p_expanded


            rTr = torch.sum(torch.conj(r) * r, dim=(1, 2, 3))
            i += 1

        return x


    def c2r(self, x):
        if x.is_complex():
            return torch.stack([x.real, x.imag], dim=-1)
        else:
            raise TypeError("Input tensor is not complex.")

# Hyperparameters
n_layers = 10
k = 3
gradient_method = 'AG'
learning_rate = 0.01
num_epochs = 10


batch_size = 64  # As defined in your DataLoader
nrow, ncol = 256, 232  # Replace with your actual image dimensions
ncoil = 12  # Replace with the number of coils in your dataset
expected_org_shape = (batch_size, nrow, ncol, 2)  # [64, 256, 232, 2]
expected_atb_shape = expected_org_shape
expected_csm_shape = (batch_size, 2, nrow, ncol)
expected_mask_shape = (batch_size, nrow, ncol)  # If mask is not complex-valued
# Create model, loss function, and optimizer
model = MoDLModel(n_layers, k, gradient_method, output_channels = 256).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


org_train, atb_train, csm_train, mask_train = getData(trnTst='training')

print(f'org_train shape: {org_train.shape}')  # Should be (batch_size, 2, height, width)
print(f'atb_train shape: {atb_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'csm_train shape: {csm_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'mask_train shape: {mask_train.shape}')
org_train_tensor = torch.from_numpy(org_train)
atb_train_tensor = torch.from_numpy(atb_train)
csm_train_tensor = torch.from_numpy(atb_train)
mask_train_tensor = torch.from_numpy(mask_train)

# print(org_train)
org_train = torch.view_as_real(org_train_tensor).to(device)
atb_train = torch.view_as_real(atb_train_tensor).to(device)
csm_train = torch.view_as_real(csm_train_tensor).to(device)
# mask_train = torch.view_as_real(mask_train_tensor).to(device)
mask_train = torch.tensor(mask_train, dtype=torch.int8).to(device)


# Convert complex MRI data and coil sensitivity maps to real-imaginary format
# org_train = c2r(org_train)  # Converts to shape [1, 2, 256, 232]
# csm_train = c2r(csm_train)  # Converts to shape [1, 12, 2, 256, 232]

# # Reshape coil sensitivity maps to flatten the coil and real-imaginary dimensions
# csm_train = csm_train.view(1, -1, 256, 232)  # New shape [1, 24, 256, 232]
#csm_train = csm_train.view(1, -1, nrow, ncol)  # Should be (1, 24, 256, 232)
csm_train = csm_train.permute(0, 3, 1, 2)  # Change from [64, 256, 232, 2] to [64, 2, 256, 232]


print(f'expected org_train shape: {expected_org_shape}')  # Should be (batch_size, 2, height, width)
print(f'expected atb_train shape: {expected_atb_shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'expected csm_train shape: {expected_csm_shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'expected mask_train shape: {expected_mask_shape}')

print(f'org_train shape: {org_train.shape}')  # Should be (batch_size, 2, height, width)
print(f'atb_train shape: {atb_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'csm_train shape: {csm_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'mask_train shape: {mask_train.shape}')
# # # Convert NumPy arrays to PyTorch tensors (if not already done)
# # org_train, atb_train, csm_train, mask_train = (
# #     torch.from_numpy(org_train),
# #     torch.from_numpy(atb_train),
# #     torch.from_numpy(csm_train),
# #     torch.from_numpy(mask_train),
# # )
# # org_val, atb_val, csm_val, mask_val = getData(trnTst='validation')
# # org_test, atb_test, csm_test, mask_test = getData(trnTst='testing')

# # Convert NumPy arrays to PyTorch tensors
# # org_train, atb_train, csm_train, mask_train = (
# #     torch.from_numpy(org_train),
# #     torch.from_numpy(atb_train),
# #     torch.from_numpy(csm_train),
# #     torch.from_numpy(mask_train),
# # )
# org_train = c2r(org_train)
# atb_train = c2r(atb_train)
# csm_train = c2r(csm_train)
# mask_train = c2r(mask_train)
# org_val, atb_val, csm_val, mask_val = (
#     torch.from_numpy(org_val),
#     torch.from_numpy(atb_val),
#     torch.from_numpy(csm_val),
#     torch.from_numpy(mask_val),
# )
# org_test, atb_test, csm_test, mask_test = (
#     torch.from_numpy(org_test),
#     torch.from_numpy(atb_test),
#     torch.from_numpy(csm_test),
#     torch.from_numpy(mask_test),
# )

# Create PyTorch Datasets

train_dataset = TensorDataset(org_train, atb_train, csm_train, mask_train)
# val_dataset = TensorDataset(org_val, atb_val, csm_val, mask_val)
# test_dataset = TensorDataset(org_test, atb_test, csm_test, mask_test)

# Create PyTorch DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Training loop
for epoch in range(num_epochs):
    model.train()
    model = model.float()

    running_loss = 0.0
    for org, atb, csm, mask in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        org, atb, csm, mask = org.to(device).float(), atb.to(device).float(), csm.to(device).float(), mask.to(device).float()
        org, atb, csm, mask = org.float(), atb.float(), csm.float(), mask.float()

        # Shape checks before feeding into the model
        assert org.shape == expected_org_shape, f"Shape mismatch for org: {org.shape}"
        assert atb.shape == expected_atb_shape, f"Shape mismatch for atb: {atb.shape}"
        assert csm.shape == expected_csm_shape, f"Shape mismatch for csm: {csm.shape}"
        assert mask.shape == expected_mask_shape, f"Shape mismatch for mask: {mask.shape}"

        optimizer.zero_grad()
        output = model(atb, csm, mask)
        loss = criterion(output, org)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    epoch_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch+1} Completed, Loss: {epoch_loss}")


#     # Validation
#     model.eval()
#     with torch.no_grad():
#         val_loss = 0.0
#         for org_val, atb_val, csm_val, mask_val in tqdm(val_loader, desc=f"Validation Epoch {epoch+1}/{num_epochs}"):
#             org_val, atb_val, csm_val, mask_val = org_val.to(device), atb_val.to(device), csm_val.to(device), mask_val.to(device)

#             val_output = model(atb_val, csm_val, mask_val)
#             val_loss += criterion(val_output, org_val).item()

#         avg_val_loss = val_loss / len(val_loader)
#         print(f"Validation Loss: {avg_val_loss}")

# # Test the model
# model.eval()
# test_loss = 0.0
# psnr_sum = 0.0
# with torch.no_grad():
#     for org_test, atb_test, csm_test, mask_test in tqdm(test_loader, desc="Testing"):
#         org_test, atb_test, csm_test, mask_test = org_test.to(device), atb_test.to(device), csm_test.to(device), mask_test.to(device)

#         test_output = model(atb_test, csm_test, mask_test)
#         test_loss += criterion(test_output, org_test).item()

#         psnr_sum += myPSNR(org_test, test_output)

# avg_test_loss = test_loss / len(test_loader)
# avg_psnr = psnr_sum / len(test_loader)
# print(f"Test Loss: {avg_test_loss}, Average PSNR: {avg_psnr}")

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.nn.parameter as Parameter

from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
import numpy as np
from datetime import datetime
from tqdm import tqdm
import time
import numpy as np
import h5py as h5
from torch.utils.data import DataLoader, TensorDataset


# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def generateUndersampled(org,csm,mask,sigma=0.05):
    nSlice,ncoil,nrow,ncol=csm.shape
    atb=np.empty(org.shape,dtype=np.complex64)
    for i in range(nSlice):
        A  = lambda z: piA(z,csm[i],mask[i],nrow,ncol,ncoil)
        At = lambda z: piAt(z,csm[i],mask[i],nrow,ncol,ncoil)

        sidx=np.where(mask[i].ravel()!=0)[0]
        nSIDX=len(sidx)
        noise=np.random.randn(nSIDX*ncoil,)+1j*np.random.randn(nSIDX*ncoil,)
        noise=noise*(sigma/np.sqrt(2.))
        y=A(org[i]) + noise
        atb[i]=At(y)
    return atb

def div0(a, b):
    """ This function handles division by zero """
    c = np.divide(a, b, out=np.zeros_like(a), where=b != 0)
    return c

def TicTocGenerator():
    # Generator that returns time differences
    ti = 0           # initial time
    tf = time.time() # final time
    while True:
        ti = tf
        tf = time.time()
        yield tf - ti  # returns the time difference

TicToc = TicTocGenerator()  # create an instance of the TicTocGen generator

def toc(tempBool=True):
    # Prints the time difference yielded by generator instance TicToc
    tempTimeInterval = next(TicToc)
    if tempBool:
        print("Elapsed time: %f seconds.\n" % tempTimeInterval)

def tic():
    # Records a time in TicToc, marks the beginning of a time interval
    toc(False)

def normalize01(img):
    """
    Normalize the image between 0 and 1
    """
    if len(img.shape) == 3:
        nimg = len(img)
    else:
        nimg = 1
        r, c = img.shape
        img = np.reshape(img, (nimg, r, c))
    img2 = np.empty(img.shape, dtype=img.dtype)
    for i in range(nimg):
        img2[i] = div0(img[i] - img[i].min(), img[i].ptp())
        # img2[i] = (img[i] - img[i].min()) / (img[i].max() - img[i].min())
    return np.squeeze(img2).astype(img.dtype)

def np_crop(data, shape=(320, 320)):
    w_from = (data.shape[-2] - shape[0]) // 2
    h_from = (data.shape[-1] - shape[1]) // 2
    w_to = w_from + shape[0]
    h_to = h_from + shape[1]
    return data[..., w_from:w_to, h_from:h_to]

def myPSNR(org, recon):
    """ This function calculates PSNR between the original and
    the reconstructed images"""
    mse = np.sum(np.square(np.abs(org - recon))) / org.size
    psnr = 20 * np.log10(org.max() / (np.sqrt(mse) + 1e-10))
    return psnr

def getData(trnTst='testing', num=100, sigma=0.01):
    print('Reading the data. Please wait...')
    filename = "/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5"  # set the correct path here
    # filename='/Users/haggarwal/datasets/piData/dataset.hdf5'

    tic()
    with h5.File(filename) as f:
        if trnTst == 'training':
            org, csm, mask = f['trnOrg'][:], f['trnCsm'][:], f['trnMask'][:]
        else:
            org, csm, mask = f['tstOrg'][num], f['tstCsm'][num], f['tstMask'][num]
            na = np.newaxis
            org, csm, mask = org[na], csm[na], mask[na]
    toc()

    print('Successfully read the data from file!')
    print('Now doing undersampling....')
    tic()
    atb = generateUndersampled(org, csm, mask, sigma)
    toc()
    print('Successfully undersampled data!')
    if trnTst == 'testing':
        atb = c2r(atb)
    return org, atb, csm, mask

def r2c(inp):
    """  input img: row x col x 2 in float32
    output image: row  x col in complex64
    """
    if inp.dtype == 'float32':
        dtype = torch.complex64
    else:
        dtype = torch.complex128
    out = torch.zeros(inp.shape[0:2], dtype=dtype)
    out = inp[..., 0] + 1j * inp[..., 1]
    return out

def c2r(inp):
    """ Convert complex PyTorch tensor to a real-imaginary format tensor """
    if inp.is_complex():
        # Convert complex tensor to real-imaginary format
        return torch.view_as_real(inp)
    else:
        # If input is not complex, return as is or handle appropriately
        return inp
    if inp.dtype == 'complex64':
        dtype = torch.float32
    else:
        dtype = torch.float64
    out = torch.zeros(inp.shape + (2,), dtype=dtype)

    # # Convert numpy arrays to PyTorch tensors before assignment
    real_tensor = torch.from_numpy(inp.real).to(dtype)
    imag_tensor = torch.from_numpy(inp.imag).to(dtype)

    out[..., 0] = real_tensor
    out[..., 1] = imag_tensor
    return out
def piA(x,csm,mask,nrow,ncol,ncoil):
    """ This is a the A operator as defined in the paper"""
    ccImg=np.reshape(x,(nrow,ncol) )
    coilImages=np.tile(ccImg,[ncoil,1,1])*csm;
    kspace=np.fft.fft2(coilImages)/np.sqrt(nrow * ncol)
    if len(mask.shape)==2:
        mask=np.tile(mask,(ncoil,1,1))
    res=kspace[mask!=0]
    return res

def piAt(kspaceUnder,csm,mask,nrow,ncol,ncoil):
    """ This is a the A^T operator as defined in the paper"""
    temp=np.zeros((ncoil,nrow,ncol),dtype=np.complex64)
    if len(mask.shape)==2:
        mask=np.tile(mask,(ncoil,1,1))

    temp[mask!=0]=kspaceUnder
    img=np.fft.ifft2(temp)*np.sqrt(nrow*ncol)
    coilComb=np.sum(img*np.conj(csm),axis=0).astype(np.complex64)
    #coilComb=coilComb.ravel();
    return coilComb

# Define the model

class ComplexConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, padding):
        super(ComplexConv2d, self).__init__()
        # Double the in_channels since real and imaginary parts are separate
        self.real_conv = nn.Conv2d(in_channels * 2, out_channels, kernel_size, padding=padding)
        self.imag_conv = nn.Conv2d(in_channels * 2, out_channels, kernel_size, padding=padding)

    def forward(self, x):
        real = self.real_conv(x)  # Real part convolution
        imag = self.imag_conv(x)  # Imaginary part convolution
        return torch.cat([real, imag], dim=1)

class ComplexReLU(nn.Module):
    def forward(self, x):
        # Apply ReLU to both real and imaginary parts
        return F.relu(x)

class DwBlock(nn.Module):
    def __init__(self, n_layers, in_channels):
        super(DwBlock, self).__init__()
        layers = [ComplexConv2d(in_channels, 128, kernel_size=3, padding=1)]
        layers += [ComplexConv2d(128, 128, kernel_size=3, padding=1) for _ in range(1, n_layers)]
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

class Aclass(nn.Module):
    def __init__(self, csm, mask, lam):
        super(Aclass, self).__init__()
        self.nrow, self.ncol = mask.shape[-2:]
        self.pixels = self.nrow * self.ncol
        self.mask = mask
        self.csm = csm
        self.SF = torch.complex(torch.sqrt(torch.tensor([self.pixels], dtype=torch.float32)), torch.tensor([0.], dtype=torch.float32))
        self.lam = lam

    def myAtA(self, img):
        print(f"Size of img: {img.size()}")
        print(f"Size of self.csm: {self.csm.size()}")
        original_shape = img.shape
        img = img.view(img.size(0), 1, 1, 1)
        coil_images = img * self.csm
        self.SF = self.SF.to(coil_images.device)
        kspace = torch.fft.ifft2(coil_images) / self.SF
        temp = kspace * self.mask[:, None, :, :]  # Add an extra dimension for broadcasting
        coil_imgs = torch.fft.fft2(temp) * self.SF
        coil_comb = torch.sum(coil_imgs * torch.conj(self.csm), axis=0)
        coil_comb = coil_comb + self.lam * img
        assert img.shape == original_shape, f"Shape altered in myAtA. Original: {original_shape}, Now: {img.shape}"
        return coil_comb

class MoDLModel(nn.Module):
    def __init__(self, n_layers, k, gradient_method):
        super(MoDLModel, self).__init__()
        self.k = k
        self.gradient_method = gradient_method
        self.dw_block = DwBlock(n_layers, in_channels=2)  # Assuming input has 2 channels (real and imaginary)

    def forward(self, atb, csm, mask):
        dc_input = atb
        for _ in range(self.k):
            dw_output = self.dw_block(dc_input)
            # Assuming lam is a scalar, expand its dimensions to match dw_output
            lam = nn.Parameter(torch.full((1, 1, 1, 1), 0.05, dtype=torch.float32)).to(dw_output.device)
            rhs = atb + lam * dw_output
            dc_input = self.dc(rhs, csm, mask, lam)
        return dc_input

    def dc(self, rhs, csm, mask, lam1):
        lam2 = lam1

        a_obj = Aclass(csm, mask, lam2)
        y = self.cg(a_obj, rhs)
        return y

    def dc_manual_gradient(self, x):
        return self.cg(x)

    def cg(self, a_obj, rhs):
        rhs = rhs.permute(0, 3, 1, 2)
        rhs = self.c2r(rhs)
        rhs = rhs.view(rhs.size(0), -1)
        rhs = rhs.permute(1, 0)
        y = self.call_cg(rhs, a_obj)
        y = y.permute(1, 0)
        y = y.view(y.size(0), 2, self.nrow, self.ncol)
        y = y.permute(0, 2, 3, 1)
        return y

    def call_cg(self, rhs, a_obj):
        y = torch.empty_like(rhs)
        for i in range(rhs.size(0)):
            a = a_obj.myAtA(rhs[i])
            y[i] = self.my_cg(a_obj, a, rhs[i])
        return y

    def my_cg(self, a_obj, a, rhs):
        x = torch.zeros_like(rhs)
        r = rhs.clone()
        p = r.clone()
        rTr = torch.sum(torch.conj(r) * r)

        i = 0
        while i < 10 and torch.any(torch.abs(rTr) > 1e-10):

            Ap = a_obj.myAtA(p)

            p_expanded = p.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(Ap)

            # Then perform the operation
            alpha = rTr / torch.sum(torch.conj(p_expanded) * Ap, dim=(1, 2, 3))

            # Reshape r to match the dimensions of Ap for broadcasting
            r_expanded = r.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(Ap)

            x = x + alpha.unsqueeze(1).unsqueeze(2).unsqueeze(3) * p
            r = r_expanded - alpha.unsqueeze(1).unsqueeze(2).unsqueeze(3) * Ap

            beta = torch.sum(torch.conj(r) * r, dim=(1, 2, 3)) / rTr
            # Reshape p to match the dimensions of r for broadcasting
            p_expanded = p.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(r)

            # Then perform the operation
            p = r + beta.unsqueeze(1).unsqueeze(2).unsqueeze(3) * p_expanded


            rTr = torch.sum(torch.conj(r) * r, dim=(1, 2, 3))
            i += 1

        return x


    def c2r(self, x):
        return torch.stack([x.real, x.imag], dim=-1)

# Hyperparameters
n_layers = 10
k = 3
gradient_method = 'AG'
learning_rate = 0.01
num_epochs = 10


batch_size = 1  # As defined in your DataLoader
nrow, ncol = 256, 232  # Replace with your actual image dimensions
ncoil = 12  # Replace with the number of coils in your dataset
expected_org_shape = (batch_size, nrow, ncol, 2)  # [64, 256, 232, 2]
expected_atb_shape = expected_org_shape
expected_csm_shape = (batch_size, 2, nrow, ncol)
expected_mask_shape = (batch_size, nrow, ncol)  # If mask is not complex-valued
# Create model, loss function, and optimizer
model = MoDLModel(n_layers, k, gradient_method).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


org_train, atb_train, csm_train, mask_train = getData(trnTst='training')

print(f'org_train shape: {org_train.shape}')  # Should be (batch_size, 2, height, width)
print(f'atb_train shape: {atb_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'csm_train shape: {csm_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'mask_train shape: {mask_train.shape}')
org_train_tensor = torch.from_numpy(org_train)
atb_train_tensor = torch.from_numpy(atb_train)
csm_train_tensor = torch.from_numpy(atb_train)
mask_train_tensor = torch.from_numpy(mask_train)

# print(org_train)
org_train = torch.view_as_real(org_train_tensor).to(device)
atb_train = torch.view_as_real(atb_train_tensor).to(device)
csm_train = torch.view_as_real(csm_train_tensor).to(device)
# mask_train = torch.view_as_real(mask_train_tensor).to(device)
mask_train = torch.tensor(mask_train, dtype=torch.int8).to(device)


# Convert complex MRI data and coil sensitivity maps to real-imaginary format
# org_train = c2r(org_train)  # Converts to shape [1, 2, 256, 232]
# csm_train = c2r(csm_train)  # Converts to shape [1, 12, 2, 256, 232]

# # Reshape coil sensitivity maps to flatten the coil and real-imaginary dimensions
# csm_train = csm_train.view(1, -1, 256, 232)  # New shape [1, 24, 256, 232]
#csm_train = csm_train.view(1, -1, nrow, ncol)  # Should be (1, 24, 256, 232)
csm_train = csm_train.permute(0, 3, 1, 2)  # Change from [64, 256, 232, 2] to [64, 2, 256, 232]


print(f'expected org_train shape: {expected_org_shape}')  # Should be (batch_size, 2, height, width)
print(f'expected atb_train shape: {expected_atb_shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'expected csm_train shape: {expected_csm_shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'expected mask_train shape: {expected_mask_shape}')

print(f'org_train shape: {org_train.shape}')  # Should be (batch_size, 2, height, width)
print(f'atb_train shape: {atb_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'csm_train shape: {csm_train.shape}')  # Should be (batch_size, num_coils * 2, height, width)
print(f'mask_train shape: {mask_train.shape}')
# # # Convert NumPy arrays to PyTorch tensors (if not already done)
# # org_train, atb_train, csm_train, mask_train = (
# #     torch.from_numpy(org_train),
# #     torch.from_numpy(atb_train),
# #     torch.from_numpy(csm_train),
# #     torch.from_numpy(mask_train),
# # )
# # org_val, atb_val, csm_val, mask_val = getData(trnTst='validation')
# # org_test, atb_test, csm_test, mask_test = getData(trnTst='testing')

# # Convert NumPy arrays to PyTorch tensors
# # org_train, atb_train, csm_train, mask_train = (
# #     torch.from_numpy(org_train),
# #     torch.from_numpy(atb_train),
# #     torch.from_numpy(csm_train),
# #     torch.from_numpy(mask_train),
# # )
# org_train = c2r(org_train)
# atb_train = c2r(atb_train)
# csm_train = c2r(csm_train)
# mask_train = c2r(mask_train)
# org_val, atb_val, csm_val, mask_val = (
#     torch.from_numpy(org_val),
#     torch.from_numpy(atb_val),
#     torch.from_numpy(csm_val),
#     torch.from_numpy(mask_val),
# )
# org_test, atb_test, csm_test, mask_test = (
#     torch.from_numpy(org_test),
#     torch.from_numpy(atb_test),
#     torch.from_numpy(csm_test),
#     torch.from_numpy(mask_test),
# )

# Create PyTorch Datasets

train_dataset = TensorDataset(org_train, atb_train, csm_train, mask_train)
# val_dataset = TensorDataset(org_val, atb_val, csm_val, mask_val)
# test_dataset = TensorDataset(org_test, atb_test, csm_test, mask_test)

# Create PyTorch DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Training loop
for epoch in range(num_epochs):
    model.train()
    model = model.float()

    running_loss = 0.0
    for org, atb, csm, mask in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        org, atb, csm, mask = org.to(device).float(), atb.to(device).float(), csm.to(device).float(), mask.to(device).float()
        org, atb, csm, mask = org.float(), atb.float(), csm.float(), mask.float()

        # Shape checks before feeding into the model
        assert org.shape == expected_org_shape, f"Shape mismatch for org: {org.shape}"
        assert atb.shape == expected_atb_shape, f"Shape mismatch for atb: {atb.shape}"
        assert csm.shape == expected_csm_shape, f"Shape mismatch for csm: {csm.shape}"
        assert mask.shape == expected_mask_shape, f"Shape mismatch for mask: {mask.shape}"

        optimizer.zero_grad()
        output = model(atb, csm, mask)
        loss = criterion(output, org)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    epoch_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch+1} Completed, Loss: {epoch_loss}")


#     # Validation
#     model.eval()
#     with torch.no_grad():
#         val_loss = 0.0
#         for org_val, atb_val, csm_val, mask_val in tqdm(val_loader, desc=f"Validation Epoch {epoch+1}/{num_epochs}"):
#             org_val, atb_val, csm_val, mask_val = org_val.to(device), atb_val.to(device), csm_val.to(device), mask_val.to(device)

#             val_output = model(atb_val, csm_val, mask_val)
#             val_loss += criterion(val_output, org_val).item()

#         avg_val_loss = val_loss / len(val_loader)
#         print(f"Validation Loss: {avg_val_loss}")

# # Test the model
# model.eval()
# test_loss = 0.0
# psnr_sum = 0.0
# with torch.no_grad():
#     for org_test, atb_test, csm_test, mask_test in tqdm(test_loader, desc="Testing"):
#         org_test, atb_test, csm_test, mask_test = org_test.to(device), atb_test.to(device), csm_test.to(device), mask_test.to(device)

#         test_output = model(atb_test, csm_test, mask_test)
#         test_loss += criterion(test_output, org_test).item()

#         psnr_sum += myPSNR(org_test, test_output)

# avg_test_loss = test_loss / len(test_loader)
# avg_psnr = psnr_sum / len(test_loader)
# print(f"Test Loss: {avg_test_loss}, Average PSNR: {avg_psnr}")

print(org_train[1])

print(org_train[1][0])

print(org_train[1][0][100])

import h5py
import torch
import matplotlib.pyplot as plt

def generateUndersampled(org,csm,mask,sigma=0.05):
    nSlice,ncoil,nrow,ncol=csm.shape
    atb=np.empty(org.shape,dtype=np.complex64)
    for i in range(nSlice):
        A  = lambda z: piA(z,csm[i],mask[i],nrow,ncol,ncoil)
        At = lambda z: piAt(z,csm[i],mask[i],nrow,ncol,ncoil)

        sidx=np.where(mask[i].ravel()!=0)[0]
        nSIDX=len(sidx)
        noise=np.random.randn(nSIDX*ncoil,)+1j*np.random.randn(nSIDX*ncoil,)
        noise=noise*(sigma/np.sqrt(2.))
        y=A(org[i]) + noise
        atb[i]=At(y)
    return atb

def getData(trnTst='testing', sigma=0.01):
    print('Reading the data. Please wait...')
    filename = "/content/drive/MyDrive/Course Project - Img and Video/Data/demoImage.hdf5"  # set the correct path here
    # filename='/Users/haggarwal/datasets/piData/dataset.hdf5'

    tic()
    with h5.File(filename) as f:
        if trnTst == 'training':
            org, csm, mask = f['trnOrg'][:], f['trnCsm'][:], f['trnMask'][:]
        else:
            org, csm, mask = f['tstOrg'], f['tstCsm'], f['tstMask']
            na = np.newaxis
            org, csm, mask = org, csm, mask
    toc()
    print('Successfully read the data from file!')
    return org, atb, csm, mask

def show_images(images, n=5):
    plt.figure(figsize=(15, 5))
    for i, image in enumerate(images[:n]):
        if image.ndim == 3:
            image = image.transpose(1, 2, 0)  # Adjusting dimensions for plotting
        plt.subplot(1, n, i + 1)
        plt.imshow(image, cmap='gray')
        plt.axis('off')
    plt.show()

# Choose 'training' or 'testing' and set the number of images
trnTst = 'testing'  # or 'training'
num = 1  # number of images to load

# Call your getData function
org, atb, csm, mask = getData(trnTst)

# Convert to PyTorch tensors


# Display images

import h5py

def explore_hdf5_file(hdf5_path):
    with h5py.File(hdf5_path, 'r') as file:
        print("Contents of the HDF5 file:")
        file.visititems(print_name_and_shape)

def print_name_and_shape(name, node):
    if isinstance(node, h5py.Dataset):
        print(f"Dataset Name: {name}")
        print(f"Dataset Shape: {node.shape}")
        print(f"Dataset Dtype: {node.dtype}")
        print("-" * 40)

# Path to your HDF5 file
hdf5_path = '/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5'

# Explore the file
explore_hdf5_file(hdf5_path)

import h5py
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
class MRIDataset(Dataset):
    def __init__(self, file_path, org_dataset, csm_dataset, mask_dataset):
        self.file = h5py.File(file_path, 'r')
        self.org_data = self.file[org_dataset]
        self.csm_data = self.file[csm_dataset]
        self.mask_data = self.file[mask_dataset]

    def __len__(self):
        return self.org_data.shape[0]

    def __getitem__(self, idx):
        org = torch.tensor(self.org_data[idx]).float()
        csm = torch.tensor(self.csm_data[idx]).float()
        mask = torch.tensor(self.mask_data[idx]).float()
        org_fft = torch.fft.fft2(org)
        undersampled = org_fft * mask
        undersampled_ifft = torch.fft.ifft2(undersampled)  # Inverse FFT to get undersampled image in spatial domain
        # Separate real and imaginary parts for model input
        undersampled_real = undersampled.real
        undersampled_imag = undersampled.imag
        return undersampled_real, undersampled_imag, org, csm, mask

# Example usage
train_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'trnOrg', 'trnCsm', 'trnMask')
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

import matplotlib.pyplot as plt

# Load a single batch or image
train_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'trnOrg', 'trnCsm', 'trnMask')
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)

# Get one sample
undersampled_real, undersampled_imag, org, csm, mask = next(iter(train_loader))

# Convert complex data to absolute values for visualization
org_abs = torch.abs(org)
csm_abs = torch.abs(csm)

# Display the original image
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.imshow(org_abs[0].numpy(), cmap='gray')
plt.title('Original Image')

# Display each coil image
for i in range(csm_abs.shape[1]):
    plt.subplot(4, 3, i + 1)  # Adjust subplot grid as needed
    plt.imshow(csm_abs[0, i].numpy(), cmap='gray')
    plt.title(f'Coil {i + 1}')

plt.show()

# Display the original image
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.imshow(org_abs[0].numpy(), cmap='gray')
plt.title('Original Image')

import matplotlib.pyplot as plt

# Get one sample
undersampled_real, undersampled_imag, org, csm, maskk = next(iter(train_loader))

# Assuming the images are complex, display the absolute values
undersampled_abs = torch.abs(undersampled_real[] + 1j*undersampled_imag)
original_abs = torch.abs(org[0])

# Display the images
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(undersampled_abs.numpy(), cmap='gray')
plt.title('Undersampled Image')

plt.subplot(1, 2, 2)
plt.imshow(original_abs.numpy(), cmap='gray')
plt.title('Original Image')
plt.show()

np.shape(undersampled.numpy())
np.shape(undersampled_abs)

np.shape(original)

def __getitem__(self, idx):
    org = torch.tensor(self.org_data[idx]).float()
    csm = torch.tensor(self.csm_data[idx]).float()
    mask = torch.tensor(self.mask_data[idx]).float()
    undersampled = org * mask  # Applying the mask for undersampling
    return undersampled, org, csm, mask



import torch.nn as nn
import torchvision.models as models
class DenseNetMRI(nn.Module):
    def __init__(self):
        super(DenseNetMRI, self).__init__()
        # Initialize a pre-trained DenseNet
        densenet = models.densenet121(pretrained=True)

        # Replace the first convolutional layer to accept 2-channel input
        # Original first conv layer: nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        densenet.features.conv0 = nn.Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)

        # Use the features from DenseNet and add additional layers
        self.features = densenet.features
        self.additional_layers = nn.Sequential(
            nn.Conv2d(1024, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Upsample(size=(128, 116)),  # Adjust size as needed
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Upsample(size=(256, 232))  # Adjust size as needed
        )

    def forward(self, x):
        x = self.features(x)
        x = self.additional_layers(x)
        return x

# Check if GPU is available and set the device accordingly
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Move the model to the specified device
model = DenseNetMRI().to(device)

# You'll need to specify the number of input and output channels
# For MRI images, this might be 1 (for grayscale) or 2 (for complex-valued)
# model = DenseNetMRI(input_channels=1, output_channels=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def calculate_psnr(output, target):
    mse = torch.mean((output - target) ** 2)
    if mse == 0:
        return float('inf')
    max_pixel = 1.0  # Assuming your images are scaled between 0 and 1
    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))
    return psnr

num_epochs = 10
for epoch in range(num_epochs):
    total_loss = 0.0
    total_psnr = 0.0
    num_batches = 0

    for undersampled_real, undersampled_imag, org, csm , mask in train_loader:
        # Concatenate real and imaginary parts along channel dimension
        undersampled_combined = torch.cat([undersampled_real.unsqueeze(1).to(device), undersampled_imag.unsqueeze(1).to(device)], dim=1)

        # Forward pass
        outputs = model(undersampled_combined)

        # Ensure 'org' has a channel dimension
        org = org.unsqueeze(1).to(device)  # Adds a channel dimension

        # Compute loss
        loss = criterion(outputs, org)
        total_loss += loss.item()

        # Compute PSNR
        psnr = calculate_psnr(outputs, org)
        total_psnr += psnr.item()

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        num_batches += 1

    # Calculate average loss and PSNR for the epoch
    avg_loss = total_loss / num_batches
    avg_psnr = total_psnr / num_batches

    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Average PSNR: {avg_psnr:.2f} dB')

torch.save(model.state_dict(), 'densenet_model.pth')
print("Model saved as densenet_model.pth")

model = DenseNetMRI()

model.load_state_dict(torch.load('densenet_model.pth'))

# Move model to the correct device (GPU or CPU)
model = model.to(device)

test_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'tstOrg', 'tstCsm', 'tstMask')
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)  # No need to shuffle for testing

def calculate_psnr(output, target):
    # Check for channel mismatch and handle accordingly
    if output.dim() == 4 and target.dim() == 3:
        # If the output has an extra dimension and it's not singleton
        if output.size(1) != 1:
            # Average across the channels or select a specific channel
            # Here, we choose to average
            output = output.mean(1)
        else:
            # If it's a singleton channel, squeeze it out
            output = output.squeeze(1)

    # Check for spatial dimension alignment
    if output.shape[1:] != target.shape[1:]:
        # Resize output to match target spatial dimensions
        output = torch.nn.functional.interpolate(output, size=(target.size(1), target.size(2)))

    mse = torch.mean((output - target) ** 2)
    if mse == 0:
        psnr = torch.tensor(float('inf'))
    else:
        max_pixel = 1.0
        psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))
    return psnr




def evaluate_model(model, test_loader, device):
    model.eval()  # Set the model to evaluation mode
    total_psnr_original = 0.0
    total_psnr_reconstructed = 0.0
    num_samples = 0
    saved_originals = []
    saved_reconstructed = []

    with torch.no_grad():  # No need to track gradients
        for undersampled_real, undersampled_imag, org, csm, mask in test_loader:
            org = org.to(device)

            # Correctly combine real and imaginary parts
            # Ensure the concatenated tensor has shape [batch_size, 2, height, width]
            undersampled_combined = torch.cat([undersampled_real.unsqueeze(1).to(device), undersampled_imag.unsqueeze(1).to(device)], dim=1)

            # Forward pass through the model
            reconstructed = model(undersampled_combined)
            print(f"reconstructed shape:{reconstructed.shape}")
            print(f"Org Shape:{org.shape}")
            # Calculate PSNR for original and reconstructed images
            psnr_original = calculate_psnr(org, org)  # PSNR with itself will be max
            psnr_reconstructed = calculate_psnr(reconstructed, org)
            saved_originals.append(org.to(device))
            saved_reconstructed.append(reconstructed.to(device))

            total_psnr_original += psnr_original.item()
            total_psnr_reconstructed += psnr_reconstructed.item()
            num_samples += 1

    avg_psnr_original = total_psnr_original / num_samples
    avg_psnr_reconstructed = total_psnr_reconstructed / num_samples

    return saved_originals, saved_reconstructed,avg_psnr_original, avg_psnr_reconstructed

saved_originals, saved_reconstructed,avg_psnr_original, avg_psnr_reconstructed = evaluate_model(model, test_loader, device)
print(f'Average PSNR for Original Images: {avg_psnr_original:.2f} dB')
print(f'Average PSNR for Reconstructed Images: {avg_psnr_reconstructed:.2f} dB')

import matplotlib.pyplot as plt

def plot_saved_images(saved_originals, saved_reconstructed, num_samples=2):
    fig, axs = plt.subplots(num_samples, 2, figsize=(10, 4 * num_samples))

    for i in range(num_samples):
        # Original image
        axs[i, 0].imshow(saved_originals[i][0].squeeze(), cmap='gray')
        axs[i, 0].set_title(f"Original Image {i+1}")
        axs[i, 0].axis('off')

        # Reconstructed image
        axs[i, 1].imshow(saved_reconstructed[i][0].squeeze(), cmap='gray')
        axs[i, 1].set_title(f"Reconstructed Image {i+1}")
        axs[i, 1].axis('off')

    plt.tight_layout()
    plt.show()

# Plot the images
plot_saved_images(saved_originals, saved_reconstructed, num_samples=2)

