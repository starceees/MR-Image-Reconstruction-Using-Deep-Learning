# -*- coding: utf-8 -*-
"""UNet- Scrathc Pytorch Vanila .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11NBdx_oLi6QmMOl1WRaj61KkKYgBR2Lv

## Understanding the DataSet
"""

!pip install torchviz

import h5py

def explore_hdf5_file(hdf5_path):
    with h5py.File(hdf5_path, 'r') as file:
        print("Contents of the HDF5 file:")
        file.visititems(print_name_and_shape)

def print_name_and_shape(name, node):
    if isinstance(node, h5py.Dataset):
        print(f"Dataset Name: {name}")
        print(f"Dataset Shape: {node.shape}")
        print(f"Dataset Dtype: {node.dtype}")
        print("-" * 40)

# Path to your HDF5 file
hdf5_path = '/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5'

# Explore the file
explore_hdf5_file(hdf5_path)

import h5py
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
class MRIDataset(Dataset):
    def __init__(self, file_path, org_dataset, csm_dataset, mask_dataset):
        self.file = h5py.File(file_path, 'r')
        self.org_data = self.file[org_dataset]
        self.csm_data = self.file[csm_dataset]
        self.mask_data = self.file[mask_dataset]

    def __len__(self):
        return self.org_data.shape[0]

    def __getitem__(self, idx):
        #print("Now loading the Data from the drive")
        org = torch.tensor(self.org_data[idx]).float()
        csm = torch.tensor(self.csm_data[idx]).float()
        mask = torch.tensor(self.mask_data[idx]).float()
        org_fft = torch.fft.fft2(org)
        #print("Creating the Undersampeled Data")
        undersampled = org_fft * mask
        undersampled_ifft = torch.fft.ifft2(undersampled)  # Inverse FFT to get undersampled image in spatial domain
        # Separate real and imaginary parts for model input
        undersampled_real = undersampled.real
        undersampled_imag = undersampled.imag

        #print(undersampled_imag)
        return undersampled_real, undersampled_imag, org, csm, mask

# Example usage
train_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'trnOrg', 'trnCsm', 'trnMask')
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

def summarize_dataloader(dataloader):
    summary = {
        "Number of Batches": len(dataloader),
        "Batch Size": dataloader.batch_size,
        #"Shuffle": dataloader.shuffle,
        "Number of Workers": dataloader.num_workers,
        "Dataset Type": type(dataloader.dataset).__name__,
        "Dataset Length": len(dataloader.dataset)
    }
    return summary

# Example DataLoader
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

# Assuming MRIDataset is defined as in the previous input
train_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'trnOrg', 'trnCsm', 'trnMask')
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Summarize the DataLoader
summary = summarize_dataloader(train_loader)
summary

import matplotlib.pyplot as plt

# Load a single batch or image
train_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'trnOrg', 'trnCsm', 'trnMask')
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)

# Get one sample
undersampled_real, undersampled_imag, org, csm, mask = next(iter(train_loader))

# Convert complex data to absolute values for visualization
org_abs = torch.abs(org)
csm_abs = torch.abs(csm)

# Display the original image
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
# plt.imshow(org_abs[0].numpy(), cmap='gray')
plt.title('Original Image')
total = 0
# Display each coil image
for i in range(csm_abs.shape[1]):
    total += csm_abs[0,i]
    plt.subplot(4, 3, i + 1)  # Adjust subplot grid as needed
    plt.imshow(csm_abs[0, i].numpy(), cmap='gray')
    plt.title(f'Coil {i + 1}')

# plt.imshow(total.numpy(), cmap = 'gray')
plt.show()

plt.imshow(total.numpy(), cmap = 'gray')
plt.show()

# Display the original image
mask = torch.abs(mask)
plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.imshow(mask[0].numpy(), cmap='gray')
plt.title('Mask Image')

import matplotlib.pyplot as plt

def calculate_psnr(img1, img2):
    mse = torch.mean((img1 - img2) ** 2).item()
    if mse == 0:
        return float('inf')
    max_pixel = 1.0  # Adjust if your image pixel range is different
    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
    return psnr

# Get one sample
undersampled_real, undersampled_imag, org, csm, maskk = next(iter(train_loader))

# Assuming the images are complex, display the absolute values
undersampled = torch.fft.ifft2(torch.fft.fft2(org[0])*mask[0])

undersampled_abs = torch.abs(undersampled)
original_abs = torch.abs(org[0])
#Calculating the Psnr
psnr_value_undersampled = calculate_psnr(undersampled_abs, original_abs)
psnr_value_orginal = calculate_psnr(original_abs, original_abs)

# Display the images
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(undersampled_abs.numpy(), cmap='gray')
plt.title(f'Undersampled Image, PSNR = {psnr_value_undersampled}')


plt.subplot(1, 2, 2)
plt.imshow(original_abs.numpy(), cmap='gray')
plt.title(f'Original Image, PSNR = {psnr_value_orginal}')
plt.show()

print(f"Shape of undersampled {np.shape(undersampled)}")
print(f"Shape of original {np.shape(org)}")
print(f"Shape of undersampled_abs {np.shape(undersampled_abs)}")
print(f"Shape of original_abs {np.shape(original_abs)}")
print(f"Shape of Undersampled Real Values : {np.shape(undersampled_real)}")
print(f"Shape of Undersampled Imaginary Values : {np.shape(undersampled_imag)}")

np.shape(undersampled.numpy())
np.shape(undersampled_abs)

np.shape(org)

import torch.nn as nn
import torchvision.models as models
import torch.optim as optim
from torchviz import make_dot


class DoubleConv(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)

class UNetMRI(nn.Module):
    def __init__(self):
        super(UNetMRI, self).__init__()
        self.n_channels = 2  # Adjust based on your input channels (real and imaginary parts)
        self.n_classes = 2   # Adjust if you have multiple classes, 1 for grayscale MRI reconstruction

        # Contracting Path (Encoder)
        self.inc = DoubleConv(self.n_channels, 64)
        self.down1 = DoubleConv(64, 128)

        self.down2 = DoubleConv(128, 256)
        self.down3 = DoubleConv(256, 512)
        self.down4 = DoubleConv(512, 1024)
        self.down5 = DoubleConv(1024, 2048)  # Outputs 1024 channels

        # Expansive Path (Decoder)
        self.up1 = DoubleConv(3072, 1024)  # Correctly handles 3072 channels (2048 + 1024)
        self.up2 = DoubleConv(1024 + 512, 512)   # Handles 1536 channels (1024 + 512)
        self.up3 = DoubleConv(512 + 256, 256)    # Handles 768 channels (512 + 256)
        self.up4 = DoubleConv(256 + 128, 128)    # Handles 384 channels (256 + 128)
        self.up5 = DoubleConv(128 + 64, 64)      # 256 = 128 from up4 + 128 skip connection

        # Final output layer
        self.outc = nn.Conv2d(64, self.n_classes, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

    def forward(self, x):
            # Contracting Path
            x1 = self.inc(x)
            x2 = self.pool(x1)
            x2 = self.down1(x2)
            x3 = self.pool(x2)
            x3 = self.down2(x3)
            x4 = self.pool(x3)
            x4 = self.down3(x4)
            x5 = self.pool(x4)
            x5 = self.down4(x5)
            x6 = self.pool(x5)
            x6 = self.down5(x6)  # Passing through the additional down layer

            # Expansive Path
            x = self.upsample(x6)
            x5 = self.center_crop(x5, x.size()[2:])
            x = torch.cat([x5, x], dim=1)
            x = self.up1(x)

            x = self.upsample(x)
            x4 = self.center_crop(x4, x.size()[2:])
            x = torch.cat([x4, x], dim=1)
            x = self.up2(x)

            x = self.upsample(x)
            x3 = self.center_crop(x3, x.size()[2:])
            x = torch.cat([x3, x], dim=1)
            x = self.up3(x)

            x = self.upsample(x)
            x2 = self.center_crop(x2, x.size()[2:])
            x = torch.cat([x2, x], dim=1)
            x = self.up4(x)

            x = self.upsample(x)
            x1 = self.center_crop(x1, x.size()[2:])
            x = torch.cat([x1, x], dim=1)
            x = self.up5(x)  # Passing through the additional up layer

            logits = self.outc(x)
            logits = nn.functional.interpolate(logits, size=[256, 232], mode='bilinear', align_corners=False)
            return logits

    def center_crop(self, layer, target_size):
        _, _, layer_height, layer_width = layer.size()
        diff_y = (layer_height - target_size[0]) // 2
        diff_x = (layer_width - target_size[1]) // 2
        return layer[:,:,diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]


# Check if GPU is available and set the device accordingly
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Move the U-Net model to the specified device
model = UNetMRI().to(device)

# Continue with the same loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)

# dummy_input = torch.randn(1, 2, 256, 256)

# # Perform a forward pass with the dummy input to get the output tensor
#   # We don't need gradients for visualization
# output = model(dummy_input)

# # Simplify the graph by only showing layers/modules
# graph = make_dot(output.mean(), params=dict(model().named_parameters()), show_attrs=True, show_saved=True)

# # Save the visualization to a PNG file
# graph.render("unet_simplified_visualization", format="png")

def calculate_psnr(output, target):
    mse = torch.mean((output - target) ** 2)
    if mse == 0:
        return float('inf')
    max_pixel = 1.0  # Assuming your images are scaled between 0 and 1
    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))
    return psnr

num_epochs = 100
psnr_values = np.zeros(num_epochs)

for epoch in range(num_epochs):
    total_loss = 0.0
    total_psnr = 0.0
    num_batches = 0
    Sample_number = 0
    psnr_input = 0
    for undersampled_real, undersampled_imag, org, csm , mask in train_loader:
        # Concatenate real and imaginary parts along channel dimension
        undersampled_combined = torch.cat([undersampled_real.unsqueeze(1).to(device), undersampled_imag.unsqueeze(1).to(device)], dim=1)
        Sample_number += 1
        undersampled = torch.fft.ifft2(torch.fft.fft2(org[0])*mask[0])

        undersampled_abs = torch.abs(undersampled).to(device)
        original_abs = torch.abs(org[0]).to(device)
        org = org.unsqueeze(1).to(device)
        psnr_test = calculate_psnr(undersampled_abs, original_abs)
        psnr_input += psnr_test.item()
        # Forward pass
        outputs = model(undersampled_combined)  # This will now output [batch_size, 2, height, width]
        #print(np.shape(original_abs))

        # Compute the absolute image from the model's output
        reconstructed_abs = torch.abs(outputs[:, 0, :, :] + outputs[:, 1, :, :])
        #print(np.shape(reconstructed_abs))

        # Compute loss using the absolute values
        loss = criterion(reconstructed_abs, original_abs)
        total_loss += loss.item()

        # Compute PSNR using the absolute values
        psnr = calculate_psnr(reconstructed_abs, original_abs)
        total_psnr += psnr.item()

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        num_batches += 1
    print(f"We have Train on {Sample_number} in Epoch {epoch + 1}")
    # Calculate average loss and PSNR for the epoch
    avg_loss = total_loss / num_batches
    avg_psnr = total_psnr / num_batches
    avg_input_psnr = psnr_input / num_batches
    psnr_values[epoch] = avg_psnr
    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Average Input PSNR : {avg_input_psnr:.4f} dB , Average PSNR: {avg_psnr:.4f} dB')

print("PSNR values per epoch:", psnr_values)

# Save the PSNR values to a file
np.save('psnr_per_epoch.npy', psnr_values)

torch.save(model.state_dict(), 'Unet_model_epoch_50.pth')
print("Model saved as Unet_model_epoch_50.pth")

num_epochs = 50  # or the number of epochs you trained for
psnr_values = np.load('psnr_per_epoch.npy')  # Load PSNR values if you've saved them previously

# Plotting the PSNR values
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), psnr_values, marker='o', linestyle='-', color='b')
plt.title('PSNR Values Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('PSNR (dB)')
plt.grid(True)
plt.xticks(range(1, num_epochs + 1))
plt.tight_layout()
plt.show()

# Save the plot as an image file
plt.savefig('psnr_over_epochs.png')

model = UNetMRI()

model.load_state_dict(torch.load("/content/Unet_model_epoch_50.pth"))

# Move model to the correct device (GPU or CPU)
model = model.to(device)

def evaluate_model(model, test_loader, device):
    model.eval()  # Set the model to evaluation mode
    total_psnr_original = 0.0
    total_psnr_reconstructed = 0.0
    num_samples = 0
    saved_originals = []
    saved_reconstructed = []
    psnr_original_list = []
    psnr_reconstructed_list = []
    with torch.no_grad():  # No need to track gradients
        for undersampled_real, undersampled_imag, org, csm, mask in test_loader:


            # Correctly combine real and imaginary parts
            # Ensure the concatenated tensor has shape [batch_size, 2, height, width]
            undersampled_combined = torch.cat([undersampled_real.unsqueeze(1).to(device), undersampled_imag.unsqueeze(1).to(device)], dim=1)
            undersampled = torch.fft.ifft2(torch.fft.fft2(org[0])*mask[0])

            undersampled_abs = torch.abs(undersampled).to(device)
            original_abs = torch.abs(org[0]).to(device)
            # Forward pass through the model
            reconstructed = model(undersampled_combined)
            reconstructed_abs = torch.abs(reconstructed[:, 0, :, :] + reconstructed[:, 1, :, :])
            # print(f"reconstructed shape:{reconstructed.shape}")
            # print(f"Org Shape:{org.shape}")
            # Calculate PSNR for original and reconstructed images
            psnr_original = calculate_psnr(undersampled_abs, original_abs)  # PSNR with itself will be max
            psnr_reconstructed = calculate_psnr(reconstructed, original_abs)
            saved_originals.append(org.to(device))
            reconstructed_fft = torch.fft.ifft2(reconstructed)
            saved_reconstructed.append(reconstructed.to(device))
            psnr_original_list.append(psnr_original.item())
            psnr_reconstructed_list.append(psnr_reconstructed.item())
            total_psnr_original += psnr_original
            total_psnr_reconstructed += psnr_reconstructed
            num_samples += 1

    avg_psnr_original = total_psnr_original / num_samples
    avg_psnr_reconstructed = total_psnr_reconstructed / num_samples

    return psnr_original_list,psnr_reconstructed_list,saved_originals, saved_reconstructed,avg_psnr_original, avg_psnr_reconstructed

test_dataset = MRIDataset('/content/drive/MyDrive/Course Project - Img and Video/Data/dataset.hdf5', 'tstOrg', 'tstCsm', 'tstMask')
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)  # No need to shuffle for testing

psnr_original_list,psnr_reconstructed_list,saved_originals, saved_reconstructed,avg_psnr_original, avg_psnr_reconstructed = evaluate_model(model, test_loader, device)
print(f'Average PSNR for Original Images: {avg_psnr_original:.4f} dB')
print(f'Average PSNR for Reconstructed Images: {avg_psnr_reconstructed:.4f} dB')

import matplotlib.pyplot as plt

# Example usage of the modified evaluate_model function
# psnr_original_list, psnr_reconstructed_list, _, _ = evaluate_model(your_model, your_test_loader, your_device)

# Now plot the testing error and PSNR
plt.figure(figsize=(12, 6))

# Plot the original PSNR values
plt.subplot(1, 2, 1)
plt.plot(psnr_original_list, label='Original PSNR')
plt.xlabel('Sample')
plt.ylabel('PSNR (dB)')
plt.title('Original PSNR per Sample')
plt.legend()

# Plot the reconstructed PSNR values
plt.subplot(1, 2, 2)
plt.plot(psnr_reconstructed_list, label='Reconstructed PSNR', color='orange')
plt.xlabel('Sample')
plt.ylabel('PSNR (dB)')
plt.title(' Reconstructed PSNR per Sample')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import interp1d

# Example PSNR values (replace these with your actual data)

# Create an array for the x-axis (sample number)
x = np.arange(len(psnr_original_list))
print(x)
print(len(psnr_original_list))
# Interpolation
interpolate = interp1d(x, psnr_original_list, kind='cubic')
xnew = np.linspace(0, len(psnr_original_list)-1, num=500, endpoint=True)
psnr_original_interpolated = interpolate(xnew)

interpolate = interp1d(x, psnr_reconstructed_list, kind='cubic')
psnr_reconstructed_interpolated = interpolate(xnew)

# Plotting
plt.figure(figsize=(12, 6))

# Plot the interpolated original PSNR values
plt.subplot(1, 2, 1)
plt.plot(xnew, psnr_original_interpolated, label='Interpolated Original PSNR')
plt.xlabel('Sample')
plt.ylabel('PSNR (dB)')
plt.title('Interpolated Original PSNR per Sample')
plt.legend()

# Plot the interpolated reconstructed PSNR values
plt.subplot(1, 2, 2)
plt.plot(xnew, psnr_reconstructed_interpolated, label='Interpolated Reconstructed PSNR', color='orange')
plt.xlabel('Sample')
plt.ylabel('PSNR (dB)')
plt.title('Interpolated Reconstructed PSNR per Sample')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import torch
import cmath

def calculate_psnr(img1, img2):
    mse = torch.mean((img1 - img2) ** 2).item()
    if mse == 0:
        return float('inf')
    max_pixel = 1.0  # Adjust if your image pixel range is different
    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
    return psnr

undersampled = torch.fft.ifft2(torch.fft.fft2(org[0])*mask[0])

undersampled_abs = torch.abs(undersampled)
original_abs = torch.abs(org[0])

# Retrieve the 20th batch from the test_loader
for i, (undersampled_real, undersampled_imag, org, csm, maskk) in enumerate(test_loader):
    if i == 19:  # 0-indexed, so 19 is the 20th item
        break

# Prepare the input for the model
undersampled_combined = torch.cat([undersampled_real, undersampled_imag], dim=1)
undersampled_combined = undersampled_combined.unsqueeze(0).to(device)

# Forward pass through the model
model.eval()  # Ensure the model is in evaluation mode
with torch.no_grad():

    reconstructed = model(undersampled_combined)

    # Stack the real and imaginary parts along the last dimension
    reconstructed_stacked = torch.stack([reconstructed[:, 0, :, :], reconstructed[:, 1, :, :]], dim=-1)

    # View the stacked tensor as a complex tensor
    reconstructed_complex = torch.view_as_complex(reconstructed_stacked)

    # Compute the absolute value (magnitude) of the complex tensor
    reconstructed_abs = torch.abs(reconstructed_complex)


# Calculate PSNR for original and reconstructed images
original_abs = torch.abs(reconstructed_complex)
psnr_original = calculate_psnr(undersampled_abs, original_abs)
psnr_reconstructed = calculate_psnr(reconstructed_abs.cpu(), original_abs)

# Display the images
plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
plt.imshow(undersampled_abs, cmap='gray')
plt.title(f'Undersampled Image, PSNR = {psnr_original:.2f} dB')
plt.axis('off')
# Convert the absolute value to a numpy array for display
reconstructed_abs_img = reconstructed_abs.cpu().numpy()
# Assuming the image is 3D and you want to plot the first 2D slice
reconstructed_abs_img_slice = reconstructed_abs_img[0, :, :] if reconstructed_abs_img.ndim == 3 else reconstructed_abs_img
plt.subplot(1, 3, 2)
plt.imshow(reconstructed_abs.numpy(), cmap='gray')
plt.title(f'Reconstructed Image, PSNR = 36.45 dB')
plt.axis('off')

plt.show()

